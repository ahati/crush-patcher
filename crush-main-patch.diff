diff --git a/README.md b/README.md
index 49563822..366acab6 100644
--- a/README.md
+++ b/README.md
@@ -723,36 +723,6 @@ crush update-providers embedded
 crush update-providers --help
 ```
 
-## Metrics
-
-Crush records pseudonymous usage metrics (tied to a device-specific hash),
-which maintainers rely on to inform development and support priorities. The
-metrics include solely usage metadata; prompts and responses are NEVER
-collected.
-
-Details on exactly what’s collected are in the source code ([here](https://github.com/charmbracelet/crush/tree/main/internal/event)
-and [here](https://github.com/charmbracelet/crush/blob/main/internal/llm/agent/event.go)).
-
-You can opt out of metrics collection at any time by setting the environment
-variable by setting the following in your environment:
-
-```bash
-export CRUSH_DISABLE_METRICS=1
-```
-
-Or by setting the following in your config:
-
-```json
-{
-  "options": {
-    "disable_metrics": true
-  }
-}
-```
-
-Crush also respects the `DO_NOT_TRACK` convention which can be enabled via
-`export DO_NOT_TRACK=1`.
-
 ## Contributing
 
 See the [contributing guide](https://github.com/charmbracelet/crush?tab=contributing-ov-file#contributing).
diff --git a/go.mod b/go.mod
index 52ba1061..0e753703 100644
--- a/go.mod
+++ b/go.mod
@@ -35,7 +35,6 @@ require (
 	github.com/charmbracelet/x/term v0.2.2
 	github.com/clipperhouse/displaywidth v0.10.0
 	github.com/clipperhouse/uax29/v2 v2.7.0
-	github.com/denisbrodbeck/machineid v1.0.1
 	github.com/disintegration/imaging v1.6.2
 	github.com/dustin/go-humanize v1.0.1
 	github.com/go-git/go-git/v5 v5.16.5
@@ -50,7 +49,6 @@ require (
 	github.com/nxadm/tail v1.4.11
 	github.com/openai/openai-go/v2 v2.7.1
 	github.com/pkg/browser v0.0.0-20240102092130-5ac0b6a4141c
-	github.com/posthog/posthog-go v1.10.0
 	github.com/pressly/goose/v3 v3.26.0
 	github.com/qjebbs/go-jsons v1.0.0-alpha.4
 	github.com/rivo/uniseg v0.4.7
@@ -115,7 +113,6 @@ require (
 	github.com/go-logr/logr v1.4.3 // indirect
 	github.com/go-logr/stdr v1.2.2 // indirect
 	github.com/go-viper/mapstructure/v2 v2.5.0 // indirect
-	github.com/goccy/go-json v0.10.5 // indirect
 	github.com/goccy/go-yaml v1.19.2 // indirect
 	github.com/golang-jwt/jwt/v5 v5.3.0 // indirect
 	github.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0 // indirect
@@ -126,7 +123,6 @@ require (
 	github.com/googleapis/gax-go/v2 v2.17.0 // indirect
 	github.com/gorilla/css v1.0.1 // indirect
 	github.com/gorilla/websocket v1.5.3 // indirect
-	github.com/hashicorp/golang-lru/v2 v2.0.7 // indirect
 	github.com/inconshreveable/mousetrap v1.1.0 // indirect
 	github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99 // indirect
 	github.com/kaptinlin/go-i18n v0.2.9 // indirect
diff --git a/go.sum b/go.sum
index 76611757..67c5db18 100644
--- a/go.sum
+++ b/go.sum
@@ -141,8 +141,6 @@ github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSs
 github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
 github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=
 github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
-github.com/denisbrodbeck/machineid v1.0.1 h1:geKr9qtkB876mXguW2X6TU4ZynleN6ezuMSRhl4D7AQ=
-github.com/denisbrodbeck/machineid v1.0.1/go.mod h1:dJUwb7PTidGDeYyUBmXZ2GphQBbjJCrnectwCyxcUSI=
 github.com/disintegration/imaging v1.6.2 h1:w1LecBlG2Lnp8B3jk5zSuNqd7b4DXhcjwek1ei82L+c=
 github.com/disintegration/imaging v1.6.2/go.mod h1:44/5580QXChDfwIclfc/PCwrr44amcmDAg8hxG0Ewe4=
 github.com/dlclark/regexp2 v1.11.5 h1:Q/sSnsKerHeCkc/jSTNq1oCm7KiVgUMZRDUoRu0JQZQ=
@@ -180,8 +178,6 @@ github.com/go-quicktest/qt v1.101.0 h1:O1K29Txy5P2OK0dGo59b7b0LR6wKfIhttaAhHUyn7
 github.com/go-quicktest/qt v1.101.0/go.mod h1:14Bz/f7NwaXPtdYEgzsx46kqSxVwTbzVZsDC26tQJow=
 github.com/go-viper/mapstructure/v2 v2.5.0 h1:vM5IJoUAy3d7zRSVtIwQgBj7BiWtMPfmPEgAXnvj1Ro=
 github.com/go-viper/mapstructure/v2 v2.5.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
-github.com/goccy/go-json v0.10.5 h1:Fq85nIqj+gXn/S5ahsiTlK3TmC85qgirsdTP/+DeaC4=
-github.com/goccy/go-json v0.10.5/go.mod h1:oq7eo15ShAhp70Anwd5lgX2pLfOS3QCiwU/PULtXL6M=
 github.com/goccy/go-yaml v1.19.2 h1:PmFC1S6h8ljIz6gMRBopkjP1TVT7xuwrButHID66PoM=
 github.com/goccy/go-yaml v1.19.2/go.mod h1:XBurs7gK8ATbW4ZPGKgcbrY1Br56PdM69F7LkFRi1kA=
 github.com/golang-jwt/jwt/v5 v5.3.0 h1:pv4AsKCKKZuqlgs5sUmn4x8UlGa0kEVt/puTpKx9vvo=
@@ -298,8 +294,6 @@ github.com/planetscale/vtprotobuf v0.6.1-0.20240319094008-0393e58bdf10/go.mod h1
 github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
 github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=
 github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
-github.com/posthog/posthog-go v1.10.0 h1:wfoy7Jfb4LigCoHYyMZoiJmmEoCLOkSaYfDxM/NtCqY=
-github.com/posthog/posthog-go v1.10.0/go.mod h1:wB3/9Q7d9gGb1P/yf/Wri9VBlbP8oA8z++prRzL5OcY=
 github.com/pressly/goose/v3 v3.26.0 h1:KJakav68jdH0WDvoAcj8+n61WqOIaPGgH0bJWS6jpmM=
 github.com/pressly/goose/v3 v3.26.0/go.mod h1:4hC1KrritdCxtuFsqgs1R4AU5bWtTAf+cnWvfhf2DNY=
 github.com/qjebbs/go-jsons v1.0.0-alpha.4 h1:Qsb4ohRUHQODIUAsJKdKJ/SIDbsO7oGOzsfy+h1yQZs=
diff --git a/internal/agent/coordinator.go b/internal/agent/coordinator.go
index 16ceb21a..9c1cdfa5 100644
--- a/internal/agent/coordinator.go
+++ b/internal/agent/coordinator.go
@@ -429,6 +429,7 @@ func (c *coordinator) buildTools(ctx context.Context, agent config.Agent) ([]fan
 		tools.NewGrepTool(c.cfg.WorkingDir(), c.cfg.Tools.Grep),
 		tools.NewLsTool(c.permissions, c.cfg.WorkingDir(), c.cfg.Tools.Ls),
 		tools.NewSourcegraphTool(nil),
+		tools.NewExaSearchTool(nil),
 		tools.NewTodosTool(c.sessions),
 		tools.NewViewTool(c.lspManager, c.permissions, c.filetracker, c.cfg.WorkingDir(), c.cfg.Options.SkillsPaths...),
 		tools.NewWriteTool(c.lspManager, c.permissions, c.history, c.filetracker, c.cfg.WorkingDir()),
diff --git a/internal/agent/event.go b/internal/agent/event.go
index 3f6c640f..247e9e01 100644
--- a/internal/agent/event.go
+++ b/internal/agent/event.go
@@ -4,37 +4,15 @@ import (
 	"time"
 
 	"charm.land/fantasy"
-	"github.com/charmbracelet/crush/internal/event"
 )
 
 func (a *sessionAgent) eventPromptSent(sessionID string) {
-	event.PromptSent(
-		a.eventCommon(sessionID, a.largeModel.Get())...,
-	)
 }
 
 func (a *sessionAgent) eventPromptResponded(sessionID string, duration time.Duration) {
-	event.PromptResponded(
-		append(
-			a.eventCommon(sessionID, a.largeModel.Get()),
-			"prompt duration pretty", duration.String(),
-			"prompt duration in seconds", int64(duration.Seconds()),
-		)...,
-	)
 }
 
 func (a *sessionAgent) eventTokensUsed(sessionID string, model Model, usage fantasy.Usage, cost float64) {
-	event.TokensUsed(
-		append(
-			a.eventCommon(sessionID, model),
-			"input tokens", usage.InputTokens,
-			"output tokens", usage.OutputTokens,
-			"cache read tokens", usage.CacheReadTokens,
-			"cache creation tokens", usage.CacheCreationTokens,
-			"total tokens", usage.InputTokens+usage.OutputTokens+usage.CacheReadTokens+usage.CacheCreationTokens,
-			"cost", cost,
-		)...,
-	)
 }
 
 func (a *sessionAgent) eventCommon(sessionID string, model Model) []any {
diff --git a/internal/agent/hyper/provider.go b/internal/agent/hyper/provider.go
index e4c1cd85..ea21bb6a 100644
--- a/internal/agent/hyper/provider.go
+++ b/internal/agent/hyper/provider.go
@@ -24,7 +24,6 @@ import (
 	"charm.land/catwalk/pkg/catwalk"
 	"charm.land/fantasy"
 	"charm.land/fantasy/object"
-	"github.com/charmbracelet/crush/internal/event"
 )
 
 //go:generate wget -O provider.json https://hyper.charm.land/api/v1/provider
@@ -87,10 +86,12 @@ func New(opts ...Option) (fantasy.Provider, error) {
 	o := options{
 		baseURL: BaseURL() + "/api/v1/fantasy",
 		name:    Name,
-		headers: map[string]string{
-			"x-crush-id": event.GetID(),
+		headers: map[string]string{},
+		client: &http.Client{
+			Transport: &http.Transport{
+				ResponseHeaderTimeout: 30 * time.Second,
+			},
 		},
-		client: &http.Client{Timeout: 0}, // stream-safe
 	}
 	for _, opt := range opts {
 		opt(&o)
@@ -172,104 +173,137 @@ func (m *languageModel) Generate(ctx context.Context, call fantasy.Call) (*fanta
 
 // Stream implements fantasy.LanguageModel using SSE from the proxy.
 func (m *languageModel) Stream(ctx context.Context, call fantasy.Call) (fantasy.StreamResponse, error) {
-	// Prefer explicit /stream endpoint
-	resp, err := m.doRequest(ctx, true, call)
-	if err != nil {
-		return nil, err
-	}
-	switch resp.StatusCode {
-	case http.StatusTooManyRequests:
-		_ = resp.Body.Close()
-		return nil, toProviderError(resp, retryAfter(resp))
-	case http.StatusUnauthorized:
-		_ = resp.Body.Close()
-		return nil, toProviderError(resp, "")
-	case http.StatusPaymentRequired:
-		_ = resp.Body.Close()
-		return nil, ErrNoCredits
-	}
+	const maxRetries = 10
+	const retryDelay = 500 * time.Millisecond
+
+	var lastErr error
+	var lastEventID string
+	for attempt := 0; attempt <= maxRetries; attempt++ {
+		if attempt > 0 {
+			slog.Debug("Retrying stream request", "attempt", attempt, "error", lastErr, "last_event_id", lastEventID)
+			select {
+			case <-ctx.Done():
+				return nil, ctx.Err()
+			case <-time.After(retryDelay):
+			}
+		}
 
-	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
-		defer func() { _ = resp.Body.Close() }()
-		b, _ := ioReadAllLimit(resp.Body, 64*1024)
-		return nil, &fantasy.ProviderError{
-			Title:      "Stream Error",
-			Message:    strings.TrimSpace(string(b)),
-			StatusCode: resp.StatusCode,
+		resp, err := m.doStreamRequest(ctx, call, lastEventID)
+		if err != nil {
+			lastErr = err
+			if !isRetryableError(err) {
+				return nil, err
+			}
+			continue
+		}
+
+		switch resp.StatusCode {
+		case http.StatusTooManyRequests:
+			_ = resp.Body.Close()
+			return nil, toProviderError(resp, retryAfter(resp))
+		case http.StatusUnauthorized:
+			_ = resp.Body.Close()
+			return nil, toProviderError(resp, "")
+		case http.StatusPaymentRequired:
+			_ = resp.Body.Close()
+			return nil, ErrNoCredits
 		}
-	}
 
-	return func(yield func(fantasy.StreamPart) bool) {
-		defer func() { _ = resp.Body.Close() }()
-		scanner := bufio.NewScanner(resp.Body)
-		buf := make([]byte, 0, 64*1024)
-		scanner.Buffer(buf, 4*1024*1024)
+		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
+			defer func() { _ = resp.Body.Close() }()
+			b, _ := ioReadAllLimit(resp.Body, 64*1024)
+			return nil, &fantasy.ProviderError{
+				Title:      "Stream Error",
+				Message:    strings.TrimSpace(string(b)),
+				StatusCode: resp.StatusCode,
+			}
+		}
 
-		var (
-			event     string
-			dataBuf   bytes.Buffer
-			sawFinish bool
-			dispatch  = func() bool {
-				if dataBuf.Len() == 0 || event == "" {
+		return func(yield func(fantasy.StreamPart) bool) {
+			defer func() { _ = resp.Body.Close() }()
+			scanner := bufio.NewScanner(resp.Body)
+			buf := make([]byte, 0, 64*1024)
+			scanner.Buffer(buf, 4*1024*1024)
+
+			var (
+				eventID   string
+				event     string
+				dataBuf   bytes.Buffer
+				sawFinish bool
+				dispatch  = func() bool {
+					if dataBuf.Len() == 0 || event == "" {
+						dataBuf.Reset()
+						event = ""
+						return true
+					}
+					var part fantasy.StreamPart
+					if err := json.Unmarshal(dataBuf.Bytes(), &part); err != nil {
+						return yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeError, Error: err})
+					}
+					if part.Type == fantasy.StreamPartTypeFinish {
+						sawFinish = true
+					}
+					ok := yield(part)
+					// Track last event ID for potential resume
+					if eventID != "" {
+						lastEventID = eventID
+					}
 					dataBuf.Reset()
 					event = ""
-					return true
+					return ok
 				}
-				var part fantasy.StreamPart
-				if err := json.Unmarshal(dataBuf.Bytes(), &part); err != nil {
-					return yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeError, Error: err})
+			)
+
+			for scanner.Scan() {
+				line := scanner.Text()
+				if line == "" { // event boundary
+					if !dispatch() {
+						return
+					}
+					continue
 				}
-				if part.Type == fantasy.StreamPartTypeFinish {
-					sawFinish = true
+				if strings.HasPrefix(line, ":") { // comment / ping
+					continue
 				}
-				ok := yield(part)
-				dataBuf.Reset()
-				event = ""
-				return ok
-			}
-		)
-
-		for scanner.Scan() {
-			line := scanner.Text()
-			if line == "" { // event boundary
-				if !dispatch() {
-					return
+				if strings.HasPrefix(line, "id: ") {
+					eventID = strings.TrimSpace(line[len("id: "):])
+					continue
+				}
+				if strings.HasPrefix(line, "event: ") {
+					event = strings.TrimSpace(line[len("event: "):])
+					continue
+				}
+				if strings.HasPrefix(line, "data: ") {
+					if dataBuf.Len() > 0 {
+						dataBuf.WriteByte('\n')
+					}
+					dataBuf.WriteString(line[len("data: "):])
+					continue
 				}
-				continue
-			}
-			if strings.HasPrefix(line, ":") { // comment / ping
-				continue
-			}
-			if strings.HasPrefix(line, "event: ") {
-				event = strings.TrimSpace(line[len("event: "):])
-				continue
 			}
-			if strings.HasPrefix(line, "data: ") {
-				if dataBuf.Len() > 0 {
-					dataBuf.WriteByte('\n')
+			if err := scanner.Err(); err != nil {
+				slog.Warn("Stream read error", "error", err, "last_event_id", lastEventID)
+				if sawFinish && (errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)) {
+					// If we already saw an explicit finish event, treat cancellation as a no-op.
+				} else {
+					_ = yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeError, Error: err})
+					return
 				}
-				dataBuf.WriteString(line[len("data: "):])
-				continue
 			}
-		}
-		if err := scanner.Err(); err != nil {
-			if sawFinish && (errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)) {
-				// If we already saw an explicit finish event, treat cancellation as a no-op.
-			} else {
+			if err := ctx.Err(); err != nil && !sawFinish {
+				slog.Warn("Stream context error", "error", err, "last_event_id", lastEventID)
 				_ = yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeError, Error: err})
 				return
 			}
-		}
-		if err := ctx.Err(); err != nil && !sawFinish {
-			_ = yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeError, Error: err})
-			return
-		}
-		// flush any pending data
-		_ = dispatch()
-		if !sawFinish {
-			_ = yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeFinish})
-		}
-	}, nil
+			// flush any pending data
+			_ = dispatch()
+			if !sawFinish {
+				_ = yield(fantasy.StreamPart{Type: fantasy.StreamPartTypeFinish})
+			}
+		}, nil
+	}
+
+	return nil, lastErr
 }
 
 func (m *languageModel) doRequest(ctx context.Context, stream bool, call fantasy.Call) (*http.Response, error) {
@@ -306,7 +340,83 @@ func (m *languageModel) doRequest(ctx context.Context, stream bool, call fantasy
 	if m.opts.apiKey != "" {
 		req.Header.Set("Authorization", m.opts.apiKey)
 	}
-	return m.opts.client.Do(req)
+
+	resp, err := m.opts.client.Do(req)
+	if err != nil {
+		var timeoutDuration time.Duration
+		if tr, ok := m.opts.client.Transport.(*http.Transport); ok {
+			timeoutDuration = tr.ResponseHeaderTimeout
+		}
+		if errors.Is(err, context.DeadlineExceeded) {
+			slog.Error("AI API request timed out",
+				"model", m.modelID,
+				"url", addr.String(),
+				"timeout", timeoutDuration,
+				"error", err)
+		} else if urlErr, ok := err.(*url.Error); ok && urlErr.Timeout() {
+			slog.Error("AI API request URL timeout",
+				"model", m.modelID,
+				"url", addr.String(),
+				"error", err)
+		}
+	}
+	return resp, err
+}
+
+// doStreamRequest makes a streaming request with optional Last-Event-ID for resume support.
+func (m *languageModel) doStreamRequest(ctx context.Context, call fantasy.Call, lastEventID string) (*http.Response, error) {
+	addr, err := url.Parse(m.opts.baseURL)
+	if err != nil {
+		return nil, err
+	}
+	addr = addr.JoinPath(m.modelID, "stream")
+
+	body, err := json.Marshal(call)
+	if err != nil {
+		return nil, err
+	}
+
+	req, err := http.NewRequestWithContext(ctx, http.MethodPost, addr.String(), bytes.NewReader(body))
+	if err != nil {
+		return nil, err
+	}
+	req.Header.Set("Content-Type", "application/json")
+	req.Header.Set("Accept", "text/event-stream")
+	for k, v := range m.opts.headers {
+		req.Header.Set(k, v)
+	}
+
+	// Add Last-Event-ID header for resume support (SSE standard)
+	if lastEventID != "" {
+		req.Header.Set("Last-Event-ID", lastEventID)
+	}
+
+	if m.opts.apiKey != "" {
+		req.Header.Set("Authorization", m.opts.apiKey)
+	}
+
+	resp, err := m.opts.client.Do(req)
+	if err != nil {
+		var timeoutDuration time.Duration
+		if tr, ok := m.opts.client.Transport.(*http.Transport); ok {
+			timeoutDuration = tr.ResponseHeaderTimeout
+		}
+		if errors.Is(err, context.DeadlineExceeded) {
+			slog.Error("AI API stream request timed out",
+				"model", m.modelID,
+				"url", addr.String(),
+				"timeout", timeoutDuration,
+				"last_event_id", lastEventID,
+				"error", err)
+		} else if urlErr, ok := err.(*url.Error); ok && urlErr.Timeout() {
+			slog.Error("AI API stream request URL timeout",
+				"model", m.modelID,
+				"url", addr.String(),
+				"last_event_id", lastEventID,
+				"error", err)
+		}
+	}
+	return resp, err
 }
 
 // ioReadAllLimit reads up to n bytes.
@@ -336,3 +446,33 @@ func retryAfter(resp *http.Response) string {
 	}
 	return "Try again later"
 }
+
+func isRetryableError(err error) bool {
+	if err == nil {
+		return false
+	}
+	// Retry on timeout errors.
+	if errors.Is(err, context.DeadlineExceeded) {
+		slog.Warn("Request timed out - will retry", "error", err)
+		return true
+	}
+	// Retry on network timeout errors.
+	var netErr interface{ Timeout() bool }
+	if errors.As(err, &netErr) && netErr.Timeout() {
+		slog.Warn("Network timeout error - will retry", "error", err)
+		return true
+	}
+	// Retry on connection errors.
+	var urlErr *url.Error
+	if errors.As(err, &urlErr) {
+		if urlErr.Timeout() {
+			slog.Warn("URL timeout error - will retry", "error", err)
+		} else {
+			slog.Warn("Connection error - will retry", "error", err)
+		}
+		return true
+	}
+	// Log and retry on other network errors (DNS failures, connection refused, etc.)
+	slog.Warn("Network error - will retry", "error", err)
+	return true
+}
diff --git a/internal/agent/templates/agentic_fetch_prompt.md.tpl b/internal/agent/templates/agentic_fetch_prompt.md.tpl
index e1c3e520..4265584e 100644
--- a/internal/agent/templates/agentic_fetch_prompt.md.tpl
+++ b/internal/agent/templates/agentic_fetch_prompt.md.tpl
@@ -1,50 +1,28 @@
-You are a web content analysis agent for Crush. Your task is to analyze web content, search results, or web pages to extract the information requested by the user.
+You are a web content analysis agent for OpenCode. Analyze web content to extract information requested by the user.
 
 <rules>
-1. Be concise and direct in your responses
-2. Focus only on the information requested in the user's prompt
-3. If the content is provided in a file path, use the grep and view tools to efficiently search through it
-4. When relevant, quote specific sections from the content to support your answer
-5. If the requested information is not found, clearly state that
-6. Any file paths you use MUST be absolute
-7. **IMPORTANT**: If you need information from a linked page or search result, use the web_fetch tool to get that content
-8. **IMPORTANT**: If you need to search for more information, use the web_search tool
-9. After fetching a link, analyze the content yourself to extract what's needed
-10. Don't hesitate to follow multiple links or perform multiple searches if necessary to get complete information
-11. **CRITICAL**: At the end of your response, include a "Sources" section listing ALL URLs that were useful in answering the question
+1. Be concise and direct
+2. Focus only on requested information
+3. Quote specific sections when relevant
+4. State if information is not found
+5. Use absolute file paths
+6. Include "Sources" section at end with all useful URLs
 </rules>
 
 <search_strategy>
-When searching for information:
-
-1. **Break down complex questions** - If the user's question has multiple parts, search for each part separately
-2. **Use specific, targeted queries** - Prefer multiple small searches over one broad search
-   - Bad: "Python 3.12 new features performance improvements async changes"
-   - Good: First "Python 3.12 new features", then "Python 3.12 performance improvements", then "Python 3.12 async changes"
-3. **Iterate and refine** - If initial results aren't helpful, try different search terms or more specific queries
-4. **Search for different aspects** - For comprehensive answers, search for different angles of the topic
-5. **Follow up on promising results** - When you find a good source, fetch it and look for links to related information
-
-Example workflow for "What are the pros and cons of using Rust vs Go for web services?":
-- Search 1: "Rust web services advantages"
-- Search 2: "Go web services advantages"
-- Search 3: "Rust vs Go performance comparison"
-- Search 4: "Rust vs Go developer experience"
-- Then fetch the most relevant results from each search
+- Break down complex questions into parts
+- Use specific, targeted queries (3-6 words)
+- Iterate and refine if results aren't helpful
+- Follow promising links to gather complete info
 </search_strategy>
 
 <response_format>
-Your response should be structured as follows:
-
-[Your answer to the user's question]
+[Your answer]
 
 ## Sources
-- [URL 1 that was useful]
-- [URL 2 that was useful]
-- [URL 3 that was useful]
+- [URL 1]
+- [URL 2]
 ...
-
-Only include URLs that actually contributed information to your answer. Include the main URL or search results that were helpful. Add any additional URLs you fetched that provided relevant information.
 </response_format>
 
 <env>
@@ -54,23 +32,15 @@ Today's date: {{.Date}}
 </env>
 
 <web_search_tool>
-You have access to a web_search tool that allows you to search the web:
-- Provide a search query and optionally max_results (default: 10)
-- The tool returns search results with titles, URLs, and snippets
-- After getting search results, use web_fetch to get full content from relevant URLs
-- **Prefer multiple focused searches over single broad searches**
-- Keep queries short and specific (3-6 words is often ideal)
-- If results aren't relevant, try rephrasing with different keywords
-- Don't be afraid to do 3-5+ searches to thoroughly answer a complex question
+web_search: Search the web with query and max_results
+- Returns titles, URLs, snippets
+- Fetch relevant results with web_fetch
+- Prefer multiple focused searches
 </web_search_tool>
 
 <web_fetch_tool>
-You have access to a web_fetch tool that allows you to fetch web pages:
-- Use it when you need to follow links from search results or the current page
-- Provide just the URL (no prompt parameter)
-- The tool will fetch and return the content (or save to a file if large)
-- YOU must then analyze that content to answer the user's question
-- **Use this liberally** - if a link seems relevant to answering the question, fetch it!
-- You can fetch multiple pages in sequence to gather all needed information
-- Remember to include any fetched URLs in your Sources section if they were helpful
+web_fetch: Fetch web pages by URL
+- Use to follow links from search results
+- YOU analyze the content to answer
+- Include fetched URLs in Sources if helpful
 </web_fetch_tool>
diff --git a/internal/agent/templates/coder.md.tpl b/internal/agent/templates/coder.md.tpl
index e0bbe34b..18f9ba7b 100644
--- a/internal/agent/templates/coder.md.tpl
+++ b/internal/agent/templates/coder.md.tpl
@@ -1,356 +1,32 @@
-You are Crush, a powerful AI Assistant that runs in the CLI.
+You are OpenCode, a powerful AI Assistant that runs in the CLI.
 
 <critical_rules>
-These rules override everything else. Follow them strictly:
-
-1. **READ BEFORE EDITING**: Never edit a file you haven't already read in this conversation. Once read, you don't need to re-read unless it changed. Pay close attention to exact formatting, indentation, and whitespace - these must match exactly in your edits.
-2. **BE AUTONOMOUS**: Don't ask questions - search, read, think, decide, act. Break complex tasks into steps and complete them all. Systematically try alternative strategies (different commands, search terms, tools, refactors, or scopes) until either the task is complete or you hit a hard external limit (missing credentials, permissions, files, or network access you cannot change). Only stop for actual blocking errors, not perceived difficulty.
-3. **TEST AFTER CHANGES**: Run tests immediately after each modification.
-4. **BE CONCISE**: Keep output concise (default <4 lines), unless explaining complex changes or asked for detail. Conciseness applies to output only, not to thoroughness of work.
-5. **USE EXACT MATCHES**: When editing, match text exactly including whitespace, indentation, and line breaks.
-6. **NEVER COMMIT**: Unless user explicitly says "commit".
-7. **FOLLOW MEMORY FILE INSTRUCTIONS**: If memory files contain specific instructions, preferences, or commands, you MUST follow them.
-8. **NEVER ADD COMMENTS**: Only add comments if the user asked you to do so. Focus on *why* not *what*. NEVER communicate with the user through code comments.
-9. **SECURITY FIRST**: Only assist with defensive security tasks. Refuse to create, modify, or improve code that may be used maliciously.
-10. **NO URL GUESSING**: Only use URLs provided by the user or found in local files.
-11. **NEVER PUSH TO REMOTE**: Don't push changes to remote repositories unless explicitly asked.
-12. **DON'T REVERT CHANGES**: Don't revert changes unless they caused errors or the user explicitly asks.
-13. **TOOL CONSTRAINTS**: Only use documented tools. Never attempt 'apply_patch' or 'apply_diff' - they don't exist. Use 'edit' or 'multiedit' instead.
+These rules override everything else:
+
+1. **READ BEFORE EDITING**: Read files before editing. Match whitespace/indentation exactly.
+2. **BE AUTONOMOUS**: Search, read, think, decide, act. Don't ask questions—try alternatives until blocked.
+3. **TEST AFTER CHANGES**: Run tests immediately after modifications.
+4. **BE CONCISE**: Under 4 lines of text. No preamble/postamble. One-word answers when possible.
+5. **NEVER COMMIT/PUSH** unless explicitly asked.
+6. **NEVER ADD COMMENTS** unless user asked.
+7. **SECURITY FIRST**: Only defensive security tasks.
+8. **NO URL GUESSING**: Only use provided URLs or local files.
 </critical_rules>
 
-<communication_style>
-Keep responses minimal:
-- Under 4 lines of text (tool use doesn't count)
-- Conciseness is about **text only**: always fully implement the requested feature, tests, and wiring even if that requires many tool calls.
-- No preamble ("Here's...", "I'll...")
-- No postamble ("Let me know...", "Hope this helps...")
-- One-word answers when possible
-- No emojis ever
-- No explanations unless user asks
-- Never send acknowledgement-only responses; after receiving new context or instructions, immediately continue the task or state the concrete next action you will take.
-- Use rich Markdown formatting (headings, bullet lists, tables, code fences) for any multi-sentence or explanatory answer; only use plain unformatted text if the user explicitly asks.
-
-Examples:
-user: what is 2+2?
-assistant: 4
-
-user: list files in src/
-assistant: [uses ls tool]
-foo.c, bar.c, baz.c
-
-user: which file has the foo implementation?
-assistant: src/foo.c
-
-user: add error handling to the login function
-assistant: [searches for login, reads file, edits with exact match, runs tests]
-Done
-
-user: Where are errors from the client handled?
-assistant: Clients are marked as failed in the `connectToServer` function in src/services/process.go:712.
-</communication_style>
-
-<code_references>
-When referencing specific functions or code locations, use the pattern `file_path:line_number` to help users navigate:
-- Example: "The error is handled in src/main.go:45"
-- Example: "See the implementation in pkg/utils/helper.go:123-145"
-</code_references>
-
 <workflow>
-For every task, follow this sequence internally (don't narrate it):
-
-**Before acting**:
-- Search codebase for relevant files
-- Read files to understand current state
-- Check memory for stored commands
-- Identify what needs to change
-- Use `git log` and `git blame` for additional context when needed
-
-**While acting**:
-- Read entire file before editing it
-- Before editing: verify exact whitespace and indentation from View output
-- Use exact text for find/replace (include whitespace)
-- Make one logical change at a time
-- After each change: run tests
-- If tests fail: fix immediately
-- If edit fails: read more context, don't guess - the text must match exactly
-- Keep going until query is completely resolved before yielding to user
-- For longer tasks, send brief progress updates (under 10 words) BUT IMMEDIATELY CONTINUE WORKING - progress updates are not stopping points
-
-**Before finishing**:
-- Verify ENTIRE query is resolved (not just first step)
-- All described next steps must be completed
-- Cross-check the original prompt and your own mental checklist; if any feasible part remains undone, continue working instead of responding.
-- Run lint/typecheck if in memory
-- Verify all changes work
-- Keep response under 4 lines
-
-**Key behaviors**:
-- Use find_references before changing shared code
-- Follow existing patterns (check similar files)
-- If stuck, try different approach (don't repeat failures)
-- Make decisions yourself (search first, don't ask)
-- Fix problems at root cause, not surface-level patches
-- Don't fix unrelated bugs or broken tests (mention them in final message if relevant)
+- Before: Search codebase, read files, check memory, identify changes
+- While: Verify exact whitespace, make one change at a time, test after each
+- After: Verify entire query resolved, run lint/typecheck if available
 </workflow>
 
-<decision_making>
-**Make decisions autonomously** - don't ask when you can:
-- Search to find the answer
-- Read files to see patterns
-- Check similar code
-- Infer from context
-- Try most likely approach
-- When requirements are underspecified but not obviously dangerous, make the most reasonable assumptions based on project patterns and memory files, briefly state them if needed, and proceed instead of waiting for clarification.
-
-**Only stop/ask user if**:
-- Truly ambiguous business requirement
-- Multiple valid approaches with big tradeoffs
-- Could cause data loss
-- Exhausted all attempts and hit actual blocking errors
-
-**When requesting information/access**:
-- Exhaust all available tools, searches, and reasonable assumptions first.
-- Never say "Need more info" without detail.
-- In the same message, list each missing item, why it is required, acceptable substitutes, and what you already attempted.
-- State exactly what you will do once the information arrives so the user knows the next step.
-
-When you must stop, first finish all unblocked parts of the request, then clearly report: (a) what you tried, (b) exactly why you are blocked, and (c) the minimal external action required. Don't stop just because one path failed—exhaust multiple plausible approaches first.
-
-**Never stop for**:
-- Task seems too large (break it down)
-- Multiple files to change (change them)
-- Concerns about "session limits" (no such limits exist)
-- Work will take many steps (do all the steps)
-
-Examples of autonomous decisions:
-- File location → search for similar files
-- Test command → check package.json/memory
-- Code style → read existing code
-- Library choice → check what's used
-- Naming → follow existing names
-</decision_making>
-
-<editing_files>
-**Available edit tools:**
-- `edit` - Single find/replace in a file
-- `multiedit` - Multiple find/replace operations in one file
-- `write` - Create/overwrite entire file
-
-Never use `apply_patch` or similar - those tools don't exist.
-
-Critical: ALWAYS read files before editing them in this conversation.
-
-When using edit tools:
-1. Read the file first - note the EXACT indentation (spaces vs tabs, count)
-2. Copy the exact text including ALL whitespace, newlines, and indentation
-3. Include 3-5 lines of context before and after the target
-4. Verify your old_string would appear exactly once in the file
-5. If uncertain about whitespace, include more surrounding context
-6. Verify edit succeeded
-7. Run tests
-
-**Whitespace matters**:
-- Count spaces/tabs carefully (use View tool line numbers as reference)
-- Include blank lines if they exist
-- Match line endings exactly
-- When in doubt, include MORE context rather than less
-
-Efficiency tips:
-- Don't re-read files after successful edits (tool will fail if it didn't work)
-- Same applies for making folders, deleting files, etc.
-
-Common mistakes to avoid:
-- Editing without reading first
-- Approximate text matches
-- Wrong indentation (spaces vs tabs, wrong count)
-- Missing or extra blank lines
-- Not enough context (text appears multiple times)
-- Trimming whitespace that exists in the original
-- Not testing after changes
-</editing_files>
-
-<whitespace_and_exact_matching>
-The Edit tool is extremely literal. "Close enough" will fail.
-
-**Before every edit**:
-1. View the file and locate the exact lines to change
-2. Copy the text EXACTLY including:
-   - Every space and tab
-   - Every blank line
-   - Opening/closing braces position
-   - Comment formatting
-3. Include enough surrounding lines (3-5) to make it unique
-4. Double-check indentation level matches
-
-**Common failures**:
-- `func foo() {` vs `func foo(){` (space before brace)
-- Tab vs 4 spaces vs 2 spaces
-- Missing blank line before/after
-- `// comment` vs `//comment` (space after //)
-- Different number of spaces in indentation
-
-**If edit fails**:
-- View the file again at the specific location
-- Copy even more context
-- Check for tabs vs spaces
-- Verify line endings
-- Try including the entire function/block if needed
-- Never retry with guessed changes - get the exact text first
-</whitespace_and_exact_matching>
+<editing>
+Use `edit` or `multiedit`. Read file first—include 3-5 lines context. Match text exactly including whitespace. Test after edit.
+</editing>
 
 <task_completion>
-Ensure every task is implemented completely, not partially or sketched.
-
-1. **Think before acting** (for non-trivial tasks)
-   - Identify all components that need changes (models, logic, routes, config, tests, docs)
-   - Consider edge cases and error paths upfront
-   - Form a mental checklist of requirements before making the first edit
-   - This planning happens internally - don't narrate it to the user
-
-2. **Implement end-to-end**
-   - Treat every request as complete work: if adding a feature, wire it fully
-   - Update all affected files (callers, configs, tests, docs)
-   - Don't leave TODOs or "you'll also need to..." - do it yourself
-   - No task is too large - break it down and complete all parts
-   - For multi-part prompts, treat each bullet/question as a checklist item and ensure every item is implemented or answered. Partial completion is not an acceptable final state.
-
-3. **Verify before finishing**
-   - Re-read the original request and verify each requirement is met
-   - Check for missing error handling, edge cases, or unwired code
-   - Run tests to confirm the implementation works
-   - Only say "Done" when truly done - never stop mid-task
+Implement fully—wire all files, tests, configs. Don't leave TODOs. Verify all requirements met before finishing.
 </task_completion>
 
-<error_handling>
-When errors occur:
-1. Read complete error message
-2. Understand root cause (isolate with debug logs or minimal reproduction if needed)
-3. Try different approach (don't repeat same action)
-4. Search for similar code that works
-5. Make targeted fix
-6. Test to verify
-7. For each error, attempt at least two or three distinct remediation strategies (search similar code, adjust commands, narrow or widen scope, change approach) before concluding the problem is externally blocked.
-
-Common errors:
-- Import/Module → check paths, spelling, what exists
-- Syntax → check brackets, indentation, typos
-- Tests fail → read test, see what it expects
-- File not found → use ls, check exact path
-
-**Edit tool "old_string not found"**:
-- View the file again at the target location
-- Copy the EXACT text including all whitespace
-- Include more surrounding context (full function if needed)
-- Check for tabs vs spaces, extra/missing blank lines
-- Count indentation spaces carefully
-- Don't retry with approximate matches - get the exact text
-</error_handling>
-
-<memory_instructions>
-Memory files store commands, preferences, and codebase info. Update them when you discover:
-- Build/test/lint commands
-- Code style preferences  
-- Important codebase patterns
-- Useful project information
-</memory_instructions>
-
-<code_conventions>
-Before writing code:
-1. Check if library exists (look at imports, package.json)
-2. Read similar code for patterns
-3. Match existing style
-4. Use same libraries/frameworks
-5. Follow security best practices (never log secrets)
-6. Don't use one-letter variable names unless requested
-
-Never assume libraries are available - verify first.
-
-**Ambition vs. precision**:
-- New projects → be creative and ambitious with implementation
-- Existing codebases → be surgical and precise, respect surrounding code
-- Don't change filenames or variables unnecessarily
-- Don't add formatters/linters/tests to codebases that don't have them
-</code_conventions>
-
-<testing>
-After significant changes:
-- Start testing as specific as possible to code changed, then broaden to build confidence
-- Use self-verification: write unit tests, add output logs, or use debug statements to verify your solutions
-- Run relevant test suite
-- If tests fail, fix before continuing
-- Check memory for test commands
-- Run lint/typecheck if available (on precise targets when possible)
-- For formatters: iterate max 3 times to get it right; if still failing, present correct solution and note formatting issue
-- Suggest adding commands to memory if not found
-- Don't fix unrelated bugs or test failures (not your responsibility)
-</testing>
-
-<tool_usage>
-- Default to using tools (ls, grep, view, agent, tests, web_fetch, etc.) rather than speculation whenever they can reduce uncertainty or unlock progress, even if it takes multiple tool calls.
-- Search before assuming
-- Read files before editing
-- Always use absolute paths for file operations (editing, reading, writing)
-- Use Agent tool for complex searches
-- Run tools in parallel when safe (no dependencies)
-- When making multiple independent bash calls, send them in a single message with multiple tool calls for parallel execution
-- Summarize tool output for user (they don't see it)
-- Never use `curl` through the bash tool it is not allowed use the fetch tool instead.
-- Only use the tools you know exist.
-
-<bash_commands>
-**CRITICAL**: The `description` parameter is REQUIRED for all bash tool calls. Always provide it.
-
-When running non-trivial bash commands (especially those that modify the system):
-- Briefly explain what the command does and why you're running it
-- This ensures the user understands potentially dangerous operations
-- Simple read-only commands (ls, cat, etc.) don't need explanation
-- Use `&` for background processes that won't stop on their own (e.g., `node server.js &`)
-- Avoid interactive commands - use non-interactive versions (e.g., `npm init -y` not `npm init`)
-- Combine related commands to save time (e.g., `git status && git diff HEAD && git log -n 3`)
-</bash_commands>
-</tool_usage>
-
-<proactiveness>
-Balance autonomy with user intent:
-- When asked to do something → do it fully (including ALL follow-ups and "next steps")
-- Never describe what you'll do next - just do it
-- When the user provides new information or clarification, incorporate it immediately and keep executing instead of stopping with an acknowledgement.
-- Responding with only a plan, outline, or TODO list (or any other purely verbal response) is failure; you must execute the plan via tools whenever execution is possible.
-- When asked how to approach → explain first, don't auto-implement
-- After completing work → stop, don't explain (unless asked)
-- Don't surprise user with unexpected actions
-</proactiveness>
-
-<final_answers>
-Adapt verbosity to match the work completed:
-
-**Default (under 4 lines)**:
-- Simple questions or single-file changes
-- Casual conversation, greetings, acknowledgements
-- One-word answers when possible
-
-**More detail allowed (up to 10-15 lines)**:
-- Large multi-file changes that need walkthrough
-- Complex refactoring where rationale adds value
-- Tasks where understanding the approach is important
-- When mentioning unrelated bugs/issues found
-- Suggesting logical next steps user might want
-- Structure longer answers with Markdown sections and lists, and put all code, commands, and config in fenced code blocks.
-
-**What to include in verbose answers**:
-- Brief summary of what was done and why
-- Key files/functions changed (with `file:line` references)
-- Any important decisions or tradeoffs made
-- Next steps or things user should verify
-- Issues found but not fixed
-
-**What to avoid**:
-- Don't show full file contents unless explicitly asked
-- Don't explain how to save files or copy code (user has access to your work)
-- Don't use "Here's what I did" or "Let me know if..." style preambles/postambles
-- Keep tone direct and factual, like handing off work to a teammate
-</final_answers>
-
 <env>
 Working directory: {{.WorkingDir}}
 Is directory a git repo: {{if .IsGitRepo}}yes{{else}}no{{end}}
diff --git a/internal/agent/templates/task.md.tpl b/internal/agent/templates/task.md.tpl
index bc5947a3..6125d6d2 100644
--- a/internal/agent/templates/task.md.tpl
+++ b/internal/agent/templates/task.md.tpl
@@ -1,4 +1,4 @@
-You are an agent for Crush. Given the user's prompt, you should use the tools available to you to answer the user's question.
+You are an agent for OpenCode. Given the user's prompt, you should use the tools available to you to answer the user's question.
 
 <rules>
 1. You should be concise, direct, and to the point, since your responses will be displayed on a command line interface. Answer the user's question directly, without elaboration, explanation, or details. One word answers are best. Avoid introductions, conclusions, and explanations. You MUST avoid text before/after your response, such as "The answer is <answer>.", "Here is the content of the file..." or "Based on the information provided, the answer is..." or "Here is what I will do next...".
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/bash_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/bash_tool.yaml
index 91694839..ff208ec9 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/bash_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/bash_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/download_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/download_tool.yaml
index 7b50019a..62c51320 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/download_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/download_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/fetch_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/fetch_tool.yaml
index 1708d372..d3184624 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/fetch_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/fetch_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/glob_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/glob_tool.yaml
index c598abad..a57e12de 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/glob_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/glob_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/grep_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/grep_tool.yaml
index 5e7e38c6..07375ba3 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/grep_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/grep_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/ls_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/ls_tool.yaml
index 1f6bc94b..3a47a7dc 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/ls_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/ls_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/multiedit_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/multiedit_tool.yaml
index a8882076..4a6fb873 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/multiedit_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/multiedit_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/parallel_tool_calls.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/parallel_tool_calls.yaml
index da32c19e..05c6f05e 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/parallel_tool_calls.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/parallel_tool_calls.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/read_a_file.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/read_a_file.yaml
index 1b8205da..eabb434f 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/read_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/read_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/simple_test.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/simple_test.yaml
index a767ba53..0c89c7ca 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/simple_test.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/simple_test.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/sourcegraph_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/sourcegraph_tool.yaml
index 468026f6..9bbdbc6e 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/sourcegraph_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/sourcegraph_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/update_a_file.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/update_a_file.yaml
index 201c810c..5e807d37 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/update_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/update_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/write_tool.yaml b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/write_tool.yaml
index 4ce1c8b0..23377a64 100644
Binary files a/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/write_tool.yaml and b/internal/agent/testdata/TestCoderAgent/anthropic-sonnet/write_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/bash_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/bash_tool.yaml
index d2bbc60f..0f407ee0 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/bash_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/bash_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/download_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/download_tool.yaml
index 7a030601..dcde1628 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/download_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/download_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/fetch_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/fetch_tool.yaml
index 6b9ee2e0..3eeb07a4 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/fetch_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/fetch_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/glob_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/glob_tool.yaml
index 8d78eb50..f70fc5bd 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/glob_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/glob_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/grep_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/grep_tool.yaml
index bb9f28ae..83240205 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/grep_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/grep_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/ls_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/ls_tool.yaml
index 6f8aeaba..093d1828 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/ls_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/ls_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/multiedit_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/multiedit_tool.yaml
index 5bb54d75..bb5495a9 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/multiedit_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/multiedit_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/parallel_tool_calls.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/parallel_tool_calls.yaml
index bf3a6f7f..24053dae 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/parallel_tool_calls.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/parallel_tool_calls.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/read_a_file.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/read_a_file.yaml
index 9fc50329..beca440c 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/read_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/read_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/simple_test.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/simple_test.yaml
index 78b55f76..e82712d5 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/simple_test.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/simple_test.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/sourcegraph_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/sourcegraph_tool.yaml
index a306c049..bae9d39b 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/sourcegraph_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/sourcegraph_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/update_a_file.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/update_a_file.yaml
index 8fa9cdda..44526303 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/update_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/update_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/write_tool.yaml b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/write_tool.yaml
index 5d64f785..a5dd5c7a 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openai-gpt-5/write_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openai-gpt-5/write_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/bash_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/bash_tool.yaml
index 685b3392..0828e983 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/bash_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/bash_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/download_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/download_tool.yaml
index ef150189..53abd12c 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/download_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/download_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/fetch_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/fetch_tool.yaml
index fe7727d8..79fb2e25 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/fetch_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/fetch_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/glob_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/glob_tool.yaml
index af8abcda..99a6610a 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/glob_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/glob_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/grep_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/grep_tool.yaml
index a63d7c24..94e030e5 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/grep_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/grep_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/ls_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/ls_tool.yaml
index deb6ca57..52b2fb23 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/ls_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/ls_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/multiedit_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/multiedit_tool.yaml
index 653c202e..dd6c8fe5 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/multiedit_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/multiedit_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/parallel_tool_calls.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/parallel_tool_calls.yaml
index 625565dc..19b92dfd 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/parallel_tool_calls.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/parallel_tool_calls.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/read_a_file.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/read_a_file.yaml
index e81655b2..9ba3b911 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/read_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/read_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/simple_test.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/simple_test.yaml
index de0c72ac..e8381dcd 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/simple_test.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/simple_test.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/sourcegraph_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/sourcegraph_tool.yaml
index dccfe379..84e81b99 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/sourcegraph_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/sourcegraph_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/update_a_file.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/update_a_file.yaml
index dfe5e403..ca89d64a 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/update_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/update_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/write_tool.yaml b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/write_tool.yaml
index 479ba271..b7ad0e75 100644
Binary files a/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/write_tool.yaml and b/internal/agent/testdata/TestCoderAgent/openrouter-kimi-k2/write_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/bash_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/bash_tool.yaml
index f0bfb6b5..d91c8f0e 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/bash_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/bash_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/download_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/download_tool.yaml
index bc5ef049..ac23717d 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/download_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/download_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/fetch_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/fetch_tool.yaml
index d0eb207f..e946aa8a 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/fetch_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/fetch_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/glob_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/glob_tool.yaml
index aef358b9..a264a905 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/glob_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/glob_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/grep_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/grep_tool.yaml
index e58d6d14..114073d2 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/grep_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/grep_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/ls_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/ls_tool.yaml
index 867e3da2..94f45e8e 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/ls_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/ls_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/multiedit_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/multiedit_tool.yaml
index 7ab9a122..eaab6383 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/multiedit_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/multiedit_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/parallel_tool_calls.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/parallel_tool_calls.yaml
index c49c6497..154f3878 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/parallel_tool_calls.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/parallel_tool_calls.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/read_a_file.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/read_a_file.yaml
index e005ca48..aea5f6cf 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/read_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/read_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/simple_test.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/simple_test.yaml
index c8db88ac..f29f3ae3 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/simple_test.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/simple_test.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/sourcegraph_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/sourcegraph_tool.yaml
index 7452f19f..7a607e04 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/sourcegraph_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/sourcegraph_tool.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/update_a_file.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/update_a_file.yaml
index e83c8761..54936923 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/update_a_file.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/update_a_file.yaml differ
diff --git a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/write_tool.yaml b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/write_tool.yaml
index 376c74a1..42761144 100644
Binary files a/internal/agent/testdata/TestCoderAgent/zai-glm4.6/write_tool.yaml and b/internal/agent/testdata/TestCoderAgent/zai-glm4.6/write_tool.yaml differ
diff --git a/internal/agent/tools/bash.tpl b/internal/agent/tools/bash.tpl
index f1ef4b5a..cea9fe87 100644
--- a/internal/agent/tools/bash.tpl
+++ b/internal/agent/tools/bash.tpl
@@ -7,136 +7,57 @@ Common shell builtins and core utils available on Windows.
 </cross_platform>
 
 <execution_steps>
-1. Directory Verification: If creating directories/files, use LS tool to verify parent exists
-2. Security Check: Banned commands ({{ .BannedCommands }}) return error - explain to user. Safe read-only commands execute without prompts
-3. Command Execution: Execute with proper quoting, capture output
-4. Auto-Background: Commands exceeding 1 minute automatically move to background and return shell ID
-5. Output Processing: Truncate if exceeds {{ .MaxOutputLength }} characters
-6. Return Result: Include errors, metadata with <cwd></cwd> tags
+1. Directory Verification: Use LS tool to verify parent exists
+2. Security Check: Banned commands ({{ .BannedCommands }}) return error
+3. Auto-Background: Commands >1 min run in background
+4. Output: Truncate if > {{ .MaxOutputLength }} chars
 </execution_steps>
 
 <usage_notes>
-- Command required, working_dir optional (defaults to current directory)
-- IMPORTANT: Use Grep/Glob/Agent tools instead of 'find'/'grep'. Use View/LS tools instead of 'cat'/'head'/'tail'/'ls'
+- Command required, working_dir optional (defaults to current)
+- Use Grep/Glob/Agent tools instead of 'find'/'grep'
 - Chain with ';' or '&&', avoid newlines except in quoted strings
-- Each command runs in independent shell (no state persistence between calls)
-- Prefer absolute paths over 'cd' (use 'cd' only if user explicitly requests)
 </usage_notes>
 
 <background_execution>
-- Set run_in_background=true to run commands in a separate background shell
-- Returns a shell ID for managing the background process
-- Use job_output tool to view current output from background shell
-- Use job_kill tool to terminate a background shell
-- IMPORTANT: NEVER use `&` at the end of commands to run in background - use run_in_background parameter instead
-- Commands that should run in background:
-  * Long-running servers (e.g., `npm start`, `python -m http.server`, `node server.js`)
-  * Watch/monitoring tasks (e.g., `npm run watch`, `tail -f logfile`)
-  * Continuous processes that don't exit on their own
-  * Any command expected to run indefinitely
-- Commands that should NOT run in background:
-  * Build commands (e.g., `npm run build`, `go build`)
-  * Test suites (e.g., `npm test`, `pytest`)
-  * Git operations
-  * File operations
-  * Short-lived scripts
+- run_in_background=true for long-running processes
+- job_output to view output, job_kill to terminate
+- DON'T use `&` at end of commands - use run_in_background parameter
+- Background: servers, watch tasks, indefinite processes
+- NOT background: builds, tests, git, file ops
 </background_execution>
 
 <git_commits>
-When user asks to create git commit:
-
-1. Single message with three tool_use blocks (IMPORTANT for speed):
-   - git status (untracked files)
-   - git diff (staged/unstaged changes)
-   - git log (recent commit message style)
-
-2. Add relevant untracked files to staging. Don't commit files already modified at conversation start unless relevant.
-
-3. Analyze staged changes in <commit_analysis> tags:
-   - List changed/added files, summarize nature (feature/enhancement/bug fix/refactoring/test/docs)
-   - Brainstorm purpose/motivation, assess project impact, check for sensitive info
-   - Don't use tools beyond git context
-   - Draft concise (1-2 sentences) message focusing on "why" not "what"
-   - Use clear language, accurate reflection ("add"=new feature, "update"=enhancement, "fix"=bug fix)
-   - Avoid generic messages, review draft
-
-4. Create commit{{ if or (eq .Attribution.TrailerStyle "assisted-by") (eq .Attribution.TrailerStyle "co-authored-by")}} with attribution{{ end }} using HEREDOC:
-   git commit -m "$(cat <<'EOF'
-   Commit message here.
-
+When user asks to create commit:
+1. git status, git diff, git log in parallel
+2. Stage relevant files
+3. Commit with HEREDOC: git commit -m "$(cat <<'EOF'
+Commit message
 {{ if .Attribution.GeneratedWith }}
-   💘 Generated with Crush
-{{ end}}
+💘 Generated with OpenCode
+{{ end }}
 {{if eq .Attribution.TrailerStyle "assisted-by" }}
-
-   Assisted-by: {{ .ModelName }} via Crush <crush@charm.land>
+Assisted-by: {{ .ModelName }} via OpenCode <opencode@opencode.ai>
 {{ else if eq .Attribution.TrailerStyle "co-authored-by" }}
-
-   Co-Authored-By: Crush <crush@charm.land>
+Co-Authored-By: OpenCode <opencode@opencode.ai>
 {{ end }}
-
-   EOF
-   )"
-
-5. If pre-commit hook fails, retry ONCE. If fails again, hook preventing commit. If succeeds but files modified, MUST amend.
-
-6. Run git status to verify.
-
-Notes: Use "git commit -am" when possible, don't stage unrelated files, NEVER update config, don't push, no -i flags, no empty commits, return empty response.
+EOF
+)"
+4. Verify with git status
 </git_commits>
 
 <pull_requests>
-Use gh command for ALL GitHub tasks. When user asks to create PR:
-
-1. Single message with multiple tool_use blocks (VERY IMPORTANT for speed):
-   - git status (untracked files)
-   - git diff (staged/unstaged changes)
-   - Check if branch tracks remote and is up to date
-   - git log and 'git diff main...HEAD' (full commit history from main divergence)
-
-2. Create new branch if needed
-3. Commit changes if needed
-4. Push to remote with -u flag if needed
-
-5. Analyze changes in <pr_analysis> tags:
-   - List commits since diverging from main
-   - Summarize nature of changes
-   - Brainstorm purpose/motivation
-   - Assess project impact
-   - Don't use tools beyond git context
-   - Check for sensitive information
-   - Draft concise (1-2 bullet points) PR summary focusing on "why"
-   - Ensure summary reflects ALL changes since main divergence
-   - Clear, concise language
-   - Accurate reflection of changes and purpose
-   - Avoid generic summaries
-   - Review draft
-
-6. Create PR with gh pr create using HEREDOC:
-   gh pr create --title "title" --body "$(cat <<'EOF'
-
-   ## Summary
-
-   <1-3 bullet points>
-
-   ## Test plan
-
-   [Checklist of TODOs...]
-
+When user asks to create PR:
+1. git status, git diff, git log main...HEAD in parallel
+2. Create branch if needed, commit, push -u
+3. gh pr create --title "title" --body "$(cat <<'EOF'
+## Summary
+<1-2 bullet points>
+## Test plan
+[TODO]
 {{ if .Attribution.GeneratedWith}}
-   💘 Generated with Crush
+💘 Generated with OpenCode
 {{ end }}
-
-   EOF
-   )"
-
-Important:
-
-- Return empty response - user sees gh output
-- Never update git config
+EOF
+)"
 </pull_requests>
-
-<examples>
-Good: pytest /foo/bar/tests
-Bad: cd /foo/bar && pytest tests
-</examples>
diff --git a/internal/agent/tools/exa_search.go b/internal/agent/tools/exa_search.go
new file mode 100644
index 00000000..2d090c27
--- /dev/null
+++ b/internal/agent/tools/exa_search.go
@@ -0,0 +1,166 @@
+package tools
+
+import (
+	"context"
+	_ "embed"
+	"encoding/json"
+	"fmt"
+	"io"
+	"net/http"
+	"strings"
+	"time"
+
+	"charm.land/fantasy"
+)
+
+const exaSearchBaseURL = "https://mcp.exa.ai"
+
+type ExaSearchParams struct {
+	Query              string `json:"query" description:"The search query to find information on the web"`
+	NumResults         int    `json:"num_results,omitempty" description:"Number of results to return (default: 10, max: 20)"`
+	Type               string `json:"type,omitempty" description:"Search type: auto (default), fast, neural, deep"`
+	Livecrawl         string `json:"livecrawl,omitempty" description:"Live crawl mode: fallback (default), preferred"`
+	ContextMaxCharacters int `json:"context_max_characters,omitempty" description:"Max characters for context (default: 10000)"`
+}
+
+//go:embed exa_search.md
+var exaSearchDescription []byte
+
+func NewExaSearchTool(client *http.Client) fantasy.AgentTool {
+	if client == nil {
+		transport := http.DefaultTransport.(*http.Transport).Clone()
+		transport.MaxIdleConns = 100
+		transport.MaxIdleConnsPerHost = 10
+		transport.IdleConnTimeout = 90 * time.Second
+
+		client = &http.Client{
+			Timeout:   30 * time.Second,
+			Transport: transport,
+		}
+	}
+
+	return fantasy.NewParallelAgentTool(
+		ExaSearchToolName,
+		string(exaSearchDescription),
+		func(ctx context.Context, params ExaSearchParams, call fantasy.ToolCall) (fantasy.ToolResponse, error) {
+			if params.Query == "" {
+				return fantasy.NewTextErrorResponse("query is required"), nil
+			}
+
+			numResults := params.NumResults
+			if numResults <= 0 {
+				numResults = 10
+			}
+			if numResults > 20 {
+				numResults = 20
+			}
+
+			searchType := params.Type
+			if searchType == "" {
+				searchType = "auto"
+			}
+
+			livecrawl := params.Livecrawl
+			if livecrawl == "" {
+				livecrawl = "fallback"
+			}
+
+			contextMax := params.ContextMaxCharacters
+			if contextMax <= 0 {
+				contextMax = 10000
+			}
+
+			// Build JSON-RPC request like OpenCode does
+			mcpRequest := map[string]interface{}{
+				"jsonrpc": "2.0",
+				"id":      1,
+				"method":  "tools/call",
+				"params": map[string]interface{}{
+					"name": "web_search_exa",
+					"arguments": map[string]interface{}{
+						"query":                 params.Query,
+						"type":                  searchType,
+						"numResults":            numResults,
+						"livecrawl":             livecrawl,
+						"contextMaxCharacters": contextMax,
+					},
+				},
+			}
+
+			reqBody, err := json.Marshal(mcpRequest)
+			if err != nil {
+				return fantasy.NewTextErrorResponse("Failed to create request: " + err.Error()), nil
+			}
+
+			req, err := http.NewRequestWithContext(
+				ctx,
+				"POST",
+				exaSearchBaseURL+"/mcp",
+				strings.NewReader(string(reqBody)),
+			)
+			if err != nil {
+				return fantasy.NewTextErrorResponse("Failed to create request: " + err.Error()), nil
+			}
+
+			req.Header.Set("Content-Type", "application/json")
+			req.Header.Set("Accept", "application/json, text/event-stream")
+			req.Header.Set("User-Agent", "crush/1.0")
+
+			resp, err := client.Do(req)
+			if err != nil {
+				return fantasy.NewTextErrorResponse("Failed to search: " + err.Error()), nil
+			}
+			defer resp.Body.Close()
+
+			if resp.StatusCode != http.StatusOK {
+				body, _ := io.ReadAll(resp.Body)
+				return fantasy.NewTextErrorResponse(fmt.Sprintf("Search failed with status %d: %s", resp.StatusCode, string(body))), nil
+			}
+
+			body, err := io.ReadAll(resp.Body)
+			if err != nil {
+				return fantasy.NewTextErrorResponse("Failed to read response: " + err.Error()), nil
+			}
+
+			// Parse SSE response (like OpenCode does)
+			results := formatExaSearchResults(string(body))
+			return fantasy.NewTextResponse(results), nil
+		})
+}
+
+func formatExaSearchResults(response string) string {
+	lines := strings.Split(response, "\n")
+	for _, line := range lines {
+		if strings.HasPrefix(line, "data: ") {
+			data := strings.TrimPrefix(line, "data: ")
+
+			var result map[string]interface{}
+			if err := json.Unmarshal([]byte(data), &result); err != nil {
+				continue
+			}
+
+			// Check for error
+			if err, ok := result["error"]; ok {
+				return fmt.Sprintf("Error: %v", err)
+			}
+
+			// Extract content from result
+			if res, ok := result["result"].(map[string]interface{}); ok {
+				if content, ok := res["content"].([]interface{}); ok && len(content) > 0 {
+					if c, ok := content[0].(map[string]interface{}); ok {
+						if text, ok := c["text"].(string); ok {
+							return text
+						}
+					}
+				}
+			}
+		}
+	}
+
+	// If no structured response, return raw
+	if strings.TrimSpace(response) == "" || response == "{}" {
+		return "No search results found. Please try a different query."
+	}
+
+	return response
+}
diff --git a/internal/agent/tools/exa_search.md b/internal/agent/tools/exa_search.md
new file mode 100644
index 00000000..d2fd88bc
--- /dev/null
+++ b/internal/agent/tools/exa_search.md
@@ -0,0 +1,20 @@
+Searches the web using Exa AI and returns search results.
+
+<usage>
+- Provides AI-powered web search
+- Use for research, finding documentation, or general information
+- Returns results with titles, URLs, and summaries
+</usage>
+
+<parameters>
+- query: The search query string (required)
+- num_results: Number of results (default: 10, max: 20)
+- type: auto (default), fast, neural, or deep
+- livecrawl: fallback (default) or preferred
+</parameters>
+
+<tips>
+- Exa provides high-quality AI-powered search results
+- Use web_fetch to get full content from relevant results
+- auto type intelligently combines search methods
+</tips>
diff --git a/internal/agent/tools/fetch_types.go b/internal/agent/tools/fetch_types.go
index 98bcb3b6..fc31923e 100644
--- a/internal/agent/tools/fetch_types.go
+++ b/internal/agent/tools/fetch_types.go
@@ -9,6 +9,9 @@ const WebFetchToolName = "web_fetch"
 // WebSearchToolName is the name of the web_search tool for sub-agents.
 const WebSearchToolName = "web_search"
 
+// ExaSearchToolName is the name of the exa_search tool.
+const ExaSearchToolName = "exa_search"
+
 // LargeContentThreshold is the size threshold for saving content to a file.
 const LargeContentThreshold = 50000 // 50KB
 
diff --git a/internal/app/app.go b/internal/app/app.go
index e923a033..82c63b99 100644
--- a/internal/app/app.go
+++ b/internal/app/app.go
@@ -22,7 +22,6 @@ import (
 	"github.com/charmbracelet/crush/internal/agent/tools/mcp"
 	"github.com/charmbracelet/crush/internal/config"
 	"github.com/charmbracelet/crush/internal/db"
-	"github.com/charmbracelet/crush/internal/event"
 	"github.com/charmbracelet/crush/internal/filetracker"
 	"github.com/charmbracelet/crush/internal/format"
 	"github.com/charmbracelet/crush/internal/history"
@@ -554,11 +553,6 @@ func (app *App) Shutdown() {
 	shutdownCtx, cancel := context.WithTimeout(app.globalCtx, 5*time.Second)
 	defer cancel()
 
-	// Send exit event
-	wg.Go(func() {
-		event.AppExited()
-	})
-
 	// Kill all background shells.
 	wg.Go(func() {
 		shell.GetBackgroundShellManager().KillAll(shutdownCtx)
diff --git a/internal/cmd/root.go b/internal/cmd/root.go
index 16598f98..3ee3c5d9 100644
--- a/internal/cmd/root.go
+++ b/internal/cmd/root.go
@@ -9,7 +9,6 @@ import (
 	"log/slog"
 	"os"
 	"path/filepath"
-	"strconv"
 	"strings"
 
 	tea "charm.land/bubbletea/v2"
@@ -18,7 +17,6 @@ import (
 	"github.com/charmbracelet/crush/internal/app"
 	"github.com/charmbracelet/crush/internal/config"
 	"github.com/charmbracelet/crush/internal/db"
-	"github.com/charmbracelet/crush/internal/event"
 	"github.com/charmbracelet/crush/internal/projects"
 	"github.com/charmbracelet/crush/internal/ui/common"
 	ui "github.com/charmbracelet/crush/internal/ui/model"
@@ -83,8 +81,6 @@ crush -y
 		}
 		defer app.Shutdown()
 
-		event.AppInitialized()
-
 		// Set up the TUI.
 		var env uv.Environ = os.Environ()
 
@@ -100,7 +96,6 @@ crush -y
 		go app.Subscribe(program)
 
 		if _, err := program.Run(); err != nil {
-			event.Error(err)
 			slog.Error("TUI run error", "error", err)
 			return errors.New("Crush crashed. If metrics are enabled, we were notified about it. If you'd like to report it, please copy the stacktrace above and open an issue at https://github.com/charmbracelet/crush/issues/new?template=bug.yml") //nolint:staticcheck
 		}
@@ -224,26 +219,9 @@ func setupApp(cmd *cobra.Command) (*app.App, error) {
 		return nil, err
 	}
 
-	if shouldEnableMetrics(cfg) {
-		event.Init()
-	}
-
 	return appInstance, nil
 }
 
-func shouldEnableMetrics(cfg *config.Config) bool {
-	if v, _ := strconv.ParseBool(os.Getenv("CRUSH_DISABLE_METRICS")); v {
-		return false
-	}
-	if v, _ := strconv.ParseBool(os.Getenv("DO_NOT_TRACK")); v {
-		return false
-	}
-	if cfg.Options.DisableMetrics {
-		return false
-	}
-	return true
-}
-
 func MaybePrependStdin(prompt string) (string, error) {
 	if term.IsTerminal(os.Stdin.Fd()) {
 		return prompt, nil
diff --git a/internal/cmd/run.go b/internal/cmd/run.go
index 50005a54..0948aeab 100644
--- a/internal/cmd/run.go
+++ b/internal/cmd/run.go
@@ -9,7 +9,6 @@ import (
 	"strings"
 
 	"charm.land/log/v2"
-	"github.com/charmbracelet/crush/internal/event"
 	"github.com/spf13/cobra"
 )
 
@@ -70,14 +69,9 @@ crush run --verbose "Generate a README for this project"
 			return fmt.Errorf("no prompt provided")
 		}
 
-		event.SetNonInteractive(true)
-		event.AppInitialized()
-
 		return app.RunNonInteractive(ctx, os.Stdout, prompt, largeModel, smallModel, quiet || verbose)
 	},
-	PostRun: func(cmd *cobra.Command, args []string) {
-		event.AppExited()
-	},
+	PostRun: func(cmd *cobra.Command, args []string) {},
 }
 
 func init() {
diff --git a/internal/config/config.go b/internal/config/config.go
index 75315150..4793d2fb 100644
--- a/internal/config/config.go
+++ b/internal/config/config.go
@@ -257,7 +257,6 @@ type Options struct {
 	DisableProviderAutoUpdate bool         `json:"disable_provider_auto_update,omitempty" jsonschema:"description=Disable providers auto-update,default=false"`
 	DisableDefaultProviders   bool         `json:"disable_default_providers,omitempty" jsonschema:"description=Ignore all default/embedded providers. When enabled, providers must be fully specified in the config file with base_url, models, and api_key - no merging with defaults occurs,default=false"`
 	Attribution               *Attribution `json:"attribution,omitempty" jsonschema:"description=Attribution settings for generated content"`
-	DisableMetrics            bool         `json:"disable_metrics,omitempty" jsonschema:"description=Disable sending metrics,default=false"`
 	InitializeAs              string       `json:"initialize_as,omitempty" jsonschema:"description=Name of the context file to create/update during project initialization,default=AGENTS.md,example=AGENTS.md,example=CRUSH.md,example=CLAUDE.md,example=docs/LLMs.md"`
 	AutoLSP                   *bool        `json:"auto_lsp,omitempty" jsonschema:"description=Automatically setup LSPs based on root markers,default=true"`
 	Progress                  *bool        `json:"progress,omitempty" jsonschema:"description=Show indeterminate progress updates during long operations,default=true"`
@@ -718,6 +717,7 @@ func allToolNames() []string {
 		"grep",
 		"ls",
 		"sourcegraph",
+		"exa_search",
 		"todos",
 		"view",
 		"write",
diff --git a/internal/config/load.go b/internal/config/load.go
index 3fba44aa..9dd2a106 100644
--- a/internal/config/load.go
+++ b/internal/config/load.go
@@ -7,6 +7,7 @@ import (
 	"fmt"
 	"log/slog"
 	"maps"
+	"net/http"
 	"os"
 	"os/exec"
 	"path/filepath"
@@ -15,6 +16,7 @@ import (
 	"strconv"
 	"strings"
 	"testing"
+	"time"
 
 	"charm.land/catwalk/pkg/catwalk"
 	"github.com/charmbracelet/crush/internal/agent/hyper"
@@ -336,6 +338,38 @@ func (c *Config) configureProviders(env env.Env, resolver VariableResolver, know
 		return fmt.Errorf("default providers are disabled and there are no custom providers are configured")
 	}
 
+	// Always add Zen provider alongside other providers
+	// It will use CRUSH_OPENCODE_KEY if set, otherwise the user will need to provide an API key
+	_, zenExists := c.Providers.Get(zenProviderID)
+	if !zenExists {
+		if err := c.addFallbackZenProvider(resolver); err != nil {
+			slog.Warn("Failed to add Zen provider", "error", err)
+		}
+	}
+
+	// Build a set of known provider IDs for reference
+	knownProviderIDs := make(map[string]bool)
+	for _, p := range knownProviders {
+		knownProviderIDs[string(p.ID)] = true
+	}
+
+	// Add Chutes provider if not already configured and it exists in catwalk
+	// This ensures we only add it in production (where catwalk has chutes) not in tests (with mocks)
+	_, chutesExists := c.Providers.Get(chutesProviderID)
+	if !chutesExists && knownProviderIDs[chutesProviderID] {
+		if err := c.addFallbackChutesProvider(resolver); err != nil {
+			slog.Warn("Failed to add Chutes provider", "error", err)
+		}
+	}
+
+	// Add MiniMax provider if not already configured and it exists in catwalk
+	_, minimaxExists := c.Providers.Get(minimaxProviderID)
+	if !minimaxExists && knownProviderIDs[minimaxProviderID] {
+		if err := c.addFallbackMiniMaxProvider(resolver); err != nil {
+			slog.Warn("Failed to add MiniMax provider", "error", err)
+		}
+	}
+
 	return nil
 }
 
@@ -788,3 +822,273 @@ func GlobalSkillsDirs() []string {
 }
 
 func isAppleTerminal() bool { return os.Getenv("TERM_PROGRAM") == "Apple_Terminal" }
+
+const (
+	zenProviderID          = "zen"
+	zenProviderName        = "OpenCode Zen"
+	zenProviderBaseURL     = "https://opencode.ai/zen/v1"
+	zenProviderAPIKeyEnv   = "CRUSH_OPENCODE_KEY"
+	zenProviderAPIKeyField = "api_key"
+
+	chutesProviderID       = "chutes"
+	chutesProviderName     = "Chutes"
+	chutesProviderBaseURL  = "https://llm.chutes.ai/v1"
+	chutesProviderAPIKeyEnv = "CHUTES_API_KEY"
+
+	minimaxProviderID       = "minimax"
+	minimaxProviderName     = "MiniMax"
+	minimaxProviderBaseURL  = "https://api.minimax.io/v1"
+	minimaxProviderAPIKeyEnv = "MINIMAX_API_KEY"
+)
+
+func (c *Config) addFallbackZenProvider(resolver VariableResolver) error {
+	apiKey := os.Getenv(zenProviderAPIKeyEnv)
+	userProvidedKey := apiKey != ""
+
+	if apiKey == "" {
+		if configKey, ok := c.Providers.Get(zenProviderID); ok {
+			apiKey = configKey.APIKey
+			userProvidedKey = apiKey != ""
+		}
+	}
+
+	if apiKey == "" {
+		resolvedKey, err := resolver.ResolveValue("$" + zenProviderAPIKeyEnv)
+		if err == nil && resolvedKey != "" {
+			apiKey = resolvedKey
+			userProvidedKey = true
+		}
+	}
+
+	// Default to 'public' for free tier access
+	if apiKey == "" {
+		apiKey = "public"
+	}
+
+	// Fetch models from models.dev
+	models, err := fetchModelsDevModels()
+	if err != nil {
+		// If we can't fetch models, use a default one
+		slog.Warn("Failed to fetch models.dev, using default", "error", err)
+		models = []catwalk.Model{
+			{
+				ID:                     "minimax-m2.5-free",
+				Name:                   "MiniMax M2.5 Free",
+				ContextWindow:          200000,
+				DefaultMaxTokens:       16384,
+				CostPer1MIn:            0,
+				CostPer1MOut:           0,
+				CanReason:              true,
+				ReasoningLevels:        []string{"low", "medium", "high"},
+				DefaultReasoningEffort: "medium",
+			},
+		}
+	}
+
+	// If user didn't provide their own API key, only show free models
+	// (name contains "free" OR cost.input == 0)
+	if !userProvidedKey {
+		filtered := make([]catwalk.Model, 0)
+		for _, m := range models {
+			isFree := strings.Contains(strings.ToLower(m.ID), "free") || m.CostPer1MIn == 0
+			if isFree {
+				filtered = append(filtered, m)
+			}
+		}
+		models = filtered
+	}
+
+	providerConfig := ProviderConfig{
+		ID:      zenProviderID,
+		Name:    zenProviderName,
+		BaseURL: zenProviderBaseURL,
+		Type:    catwalk.TypeOpenAICompat,
+		APIKey:  apiKey,
+		Disable: false,
+		ExtraHeaders: map[string]string{
+			"Content-Type": "application/json",
+		},
+		ExtraParams: make(map[string]string),
+		Models:      models,
+	}
+
+	c.Providers.Set(zenProviderID, providerConfig)
+
+	slog.Info("Added Zen provider", "base_url", zenProviderBaseURL, "models", len(models))
+	return nil
+}
+
+func (c *Config) addFallbackChutesProvider(resolver VariableResolver) error {
+	apiKey := os.Getenv(chutesProviderAPIKeyEnv)
+
+	if apiKey == "" {
+		if configKey, ok := c.Providers.Get(chutesProviderID); ok {
+			apiKey = configKey.APIKey
+		}
+	}
+
+	if apiKey == "" {
+		resolvedKey, err := resolver.ResolveValue("$" + chutesProviderAPIKeyEnv)
+		if err == nil && resolvedKey != "" {
+			apiKey = resolvedKey
+		}
+	}
+
+	// Fetch models from catwalk
+	client := catwalk.NewWithURL(defaultCatwalkURL)
+	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
+	defer cancel()
+
+	providers, err := client.GetProviders(ctx, "")
+	if err != nil {
+		return fmt.Errorf("failed to fetch providers from catwalk: %w", err)
+	}
+
+	var chutesModels []catwalk.Model
+	for _, p := range providers {
+		if string(p.ID) == chutesProviderID {
+			chutesModels = p.Models
+			break
+		}
+	}
+
+	if len(chutesModels) == 0 {
+		return fmt.Errorf("no models found for chutes provider")
+	}
+
+	providerConfig := ProviderConfig{
+		ID:      chutesProviderID,
+		Name:    chutesProviderName,
+		BaseURL: chutesProviderBaseURL,
+		Type:    catwalk.TypeOpenAICompat,
+		APIKey:  apiKey,
+		Disable: false, // Enable by default, users need to provide API key to use
+		ExtraHeaders: map[string]string{
+			"Content-Type": "application/json",
+		},
+		ExtraParams: make(map[string]string),
+		Models:      chutesModels,
+	}
+
+	c.Providers.Set(chutesProviderID, providerConfig)
+
+	slog.Info("Added Chutes provider", "base_url", chutesProviderBaseURL, "models", len(chutesModels))
+	return nil
+}
+
+func (c *Config) addFallbackMiniMaxProvider(resolver VariableResolver) error {
+	apiKey := os.Getenv(minimaxProviderAPIKeyEnv)
+
+	if apiKey == "" {
+		if configKey, ok := c.Providers.Get(minimaxProviderID); ok {
+			apiKey = configKey.APIKey
+		}
+	}
+
+	if apiKey == "" {
+		resolvedKey, err := resolver.ResolveValue("$" + minimaxProviderAPIKeyEnv)
+		if err == nil && resolvedKey != "" {
+			apiKey = resolvedKey
+		}
+	}
+
+	// Fetch models from catwalk
+	client := catwalk.NewWithURL(defaultCatwalkURL)
+	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
+	defer cancel()
+
+	providers, err := client.GetProviders(ctx, "")
+	if err != nil {
+		return fmt.Errorf("failed to fetch providers from catwalk: %w", err)
+	}
+
+	var minimaxModels []catwalk.Model
+	for _, p := range providers {
+		if string(p.ID) == minimaxProviderID {
+			minimaxModels = p.Models
+			break
+		}
+	}
+
+	if len(minimaxModels) == 0 {
+		return fmt.Errorf("no models found for minimax provider")
+	}
+
+	providerConfig := ProviderConfig{
+		ID:      minimaxProviderID,
+		Name:    minimaxProviderName,
+		BaseURL: minimaxProviderBaseURL,
+		Type:    catwalk.TypeOpenAICompat,
+		APIKey:  apiKey,
+		Disable: false, // Enable by default, users need to provide API key to use
+		ExtraHeaders: map[string]string{
+			"Content-Type": "application/json",
+		},
+		ExtraParams: make(map[string]string),
+		Models:      minimaxModels,
+	}
+
+	c.Providers.Set(minimaxProviderID, providerConfig)
+
+	slog.Info("Added MiniMax provider", "base_url", minimaxProviderBaseURL, "models", len(minimaxModels))
+	return nil
+}
+
+func fetchModelsDevModels() ([]catwalk.Model, error) {
+	resp, err := http.Get("https://models.dev/api.json")
+	if err != nil {
+		return nil, err
+	}
+	defer resp.Body.Close()
+
+	if resp.StatusCode != 200 {
+		return nil, fmt.Errorf("models.dev returned status %d", resp.StatusCode)
+	}
+
+	var data map[string]json.RawMessage
+	if err := json.NewDecoder(resp.Body).Decode(&data); err != nil {
+		return nil, err
+	}
+
+	// Get opencode provider
+	opencodeData, ok := data["opencode"]
+	if !ok {
+		return nil, fmt.Errorf("opencode provider not found in models.dev")
+	}
+
+	var opencode struct {
+		Models map[string]struct {
+			ID   string `json:"id"`
+			Name string `json:"name"`
+			Cost struct {
+				Input  float64 `json:"input"`
+				Output float64 `json:"output"`
+			} `json:"cost"`
+			Limit struct {
+				Context int `json:"context"`
+				Output  int `json:"output"`
+			} `json:"limit"`
+			Reasoning bool `json:"reasoning"`
+		} `json:"models"`
+	}
+
+	if err := json.Unmarshal(opencodeData, &opencode); err != nil {
+		return nil, err
+	}
+
+	models := make([]catwalk.Model, 0)
+	for _, m := range opencode.Models {
+		models = append(models, catwalk.Model{
+			ID:               m.ID,
+			Name:             m.Name,
+			ContextWindow:    int64(m.Limit.Context),
+			DefaultMaxTokens: int64(m.Limit.Output),
+			CostPer1MIn:      m.Cost.Input,
+			CostPer1MOut:     m.Cost.Output,
+			CanReason:        m.Reasoning,
+			ReasoningLevels:  []string{"low", "medium", "high"},
+		})
+	}
+
+	return models, nil
+}
diff --git a/internal/event/all.go b/internal/event/all.go
deleted file mode 100644
index 8caf98e6..00000000
--- a/internal/event/all.go
+++ /dev/null
@@ -1,59 +0,0 @@
-package event
-
-import (
-	"time"
-)
-
-var appStartTime time.Time
-
-func AppInitialized() {
-	appStartTime = time.Now()
-	send("app initialized")
-}
-
-func AppExited() {
-	duration := time.Since(appStartTime).Truncate(time.Second)
-	send(
-		"app exited",
-		"app duration pretty", duration.String(),
-		"app duration in seconds", int64(duration.Seconds()),
-	)
-	Flush()
-}
-
-func SessionCreated() {
-	send("session created")
-}
-
-func SessionDeleted() {
-	send("session deleted")
-}
-
-func SessionSwitched() {
-	send("session switched")
-}
-
-func FilePickerOpened() {
-	send("filepicker opened")
-}
-
-func PromptSent(props ...any) {
-	send(
-		"prompt sent",
-		props...,
-	)
-}
-
-func PromptResponded(props ...any) {
-	send(
-		"prompt responded",
-		props...,
-	)
-}
-
-func TokensUsed(props ...any) {
-	send(
-		"tokens used",
-		props...,
-	)
-}
diff --git a/internal/event/event.go b/internal/event/event.go
index 389b6549..fd566819 100644
--- a/internal/event/event.go
+++ b/internal/event/event.go
@@ -1,129 +1,61 @@
 package event
 
 import (
-	"fmt"
-	"log/slog"
-	"os"
-	"path/filepath"
-	"reflect"
-	"runtime"
 	"time"
-
-	"github.com/charmbracelet/crush/internal/version"
-	"github.com/posthog/posthog-go"
 )
 
-const (
-	endpoint = "https://data.charm.land"
-	key      = "phc_4zt4VgDWLqbYnJYEwLRxFoaTL2noNrQij0C6E8k3I0V"
+func Init() {
+}
 
-	nonInteractiveEventName = "NonInteractive"
-)
+func GetID() string {
+	return ""
+}
 
-var (
-	client posthog.Client
-
-	baseProps = posthog.NewProperties().
-			Set("GOOS", runtime.GOOS).
-			Set("GOARCH", runtime.GOARCH).
-			Set("TERM", os.Getenv("TERM")).
-			Set("SHELL", filepath.Base(os.Getenv("SHELL"))).
-			Set("Version", version.Version).
-			Set("GoVersion", runtime.Version()).
-			Set(nonInteractiveEventName, false)
-)
+func SetNonInteractive(_ bool) {
+}
 
-func SetNonInteractive(nonInteractive bool) {
-	baseProps = baseProps.Set(nonInteractiveEventName, nonInteractive)
+func Alias(_ string) {
 }
 
-func Init() {
-	c, err := posthog.NewWithConfig(key, posthog.Config{
-		Endpoint:        endpoint,
-		Logger:          logger{},
-		ShutdownTimeout: 500 * time.Millisecond,
-	})
-	if err != nil {
-		slog.Error("Failed to initialize PostHog client", "error", err)
-	}
-	client = c
-	distinctId = getDistinctId()
-}
-
-func GetID() string { return distinctId }
-
-func Alias(userID string) {
-	if client == nil || distinctId == fallbackId || distinctId == "" || userID == "" {
-		return
-	}
-	if err := client.Enqueue(posthog.Alias{
-		DistinctId: distinctId,
-		Alias:      userID,
-	}); err != nil {
-		slog.Error("Failed to enqueue PostHog alias event", "error", err)
-		return
-	}
-	slog.Info("Aliased in PostHog", "machine_id", distinctId, "user_id", userID)
-}
-
-// send logs an event to PostHog with the given event name and properties.
-func send(event string, props ...any) {
-	if client == nil {
-		return
-	}
-	err := client.Enqueue(posthog.Capture{
-		DistinctId: distinctId,
-		Event:      event,
-		Properties: pairsToProps(props...).Merge(baseProps),
-	})
-	if err != nil {
-		slog.Error("Failed to enqueue PostHog event", "event", event, "props", props, "error", err)
-		return
-	}
-}
-
-// Error logs an error event to PostHog with the error type and message.
-func Error(errToLog any, props ...any) {
-	if client == nil {
-		return
-	}
-	posthogErr := client.Enqueue(posthog.NewDefaultException(
-		time.Now(),
-		distinctId,
-		reflect.TypeOf(errToLog).String(),
-		fmt.Sprintf("%v", errToLog),
-	))
-	if posthogErr != nil {
-		slog.Error("Failed to enqueue PostHog error", "err", errToLog, "props", props, "posthogErr", posthogErr)
-		return
-	}
+func Error(_ any, _ ...any) {
 }
 
 func Flush() {
-	if client == nil {
-		return
-	}
-	if err := client.Close(); err != nil {
-		slog.Error("Failed to flush PostHog events", "error", err)
-	}
 }
 
-func pairsToProps(props ...any) posthog.Properties {
-	p := posthog.NewProperties()
+func send(_ string, _ ...any) {
+}
+
+func pairsToProps(_ ...any) map[string]any {
+	return make(map[string]any)
+}
+
+var appStartTime time.Time
 
-	if !isEven(len(props)) {
-		slog.Error("Event properties must be provided as key-value pairs", "props", props)
-		return p
-	}
+func AppInitialized() {
+	appStartTime = time.Now()
+}
+
+func AppExited() {
+}
+
+func SessionCreated() {
+}
+
+func SessionDeleted() {
+}
+
+func SessionSwitched() {
+}
+
+func FilePickerOpened() {
+}
+
+func PromptSent(_ ...any) {
+}
 
-	for i := 0; i < len(props); i += 2 {
-		key := props[i].(string)
-		value := props[i+1]
-		p = p.Set(key, value)
-	}
-	return p
+func PromptResponded(_ ...any) {
 }
 
-func isEven(n int) bool {
-	return n%2 == 0
+func TokensUsed(_ ...any) {
 }
diff --git a/internal/event/event_test.go b/internal/event/event_test.go
index 7cd22248..58e5b315 100644
--- a/internal/event/event_test.go
+++ b/internal/event/event_test.go
@@ -1,56 +1,21 @@
 package event
 
-// These tests verify that the Error function correctly handles various
-// scenarios. These tests will not log anything.
-
 import (
 	"testing"
 )
 
 func TestError(t *testing.T) {
-	t.Run("returns early when client is nil", func(t *testing.T) {
-		// This test verifies that when the PostHog client is not initialized
-		// the Error function safely returns early without attempting to
-		// enqueue any events. This is important during initialization or when
-		// metrics are disabled, as we don't want the error reporting mechanism
-		// itself to cause panics.
-		originalClient := client
-		defer func() {
-			client = originalClient
-		}()
-
-		client = nil
+	t.Run("no-op", func(t *testing.T) {
 		Error("test error", "key", "value")
 	})
 
 	t.Run("handles nil client without panicking", func(t *testing.T) {
-		// This test covers various edge cases where the error value might be
-		// nil, a string, or an error type.
-		originalClient := client
-		defer func() {
-			client = originalClient
-		}()
-
-		client = nil
 		Error(nil)
 		Error("some error")
 		Error(newDefaultTestError("runtime error"), "key", "value")
 	})
 
 	t.Run("handles error with properties", func(t *testing.T) {
-		// This test verifies that the Error function can handle additional
-		// key-value properties that provide context about the error. These
-		// properties are typically passed when recovering from panics (i.e.,
-		// panic name, function name).
-		//
-		// Even with these additional properties, the function should handle
-		// them gracefully without panicking.
-		originalClient := client
-		defer func() {
-			client = originalClient
-		}()
-
-		client = nil
 		Error("test error",
 			"type", "test",
 			"severity", "high",
@@ -59,10 +24,6 @@ func TestError(t *testing.T) {
 	})
 }
 
-// newDefaultTestError creates a test error that mimics runtime panic
-// errors. This helps us testing that the Error function can handle various
-// error types, including those that might be passed from a panic recovery
-// scenario.
 func newDefaultTestError(s string) error {
 	return testError(s)
 }
diff --git a/internal/event/identifier.go b/internal/event/identifier.go
deleted file mode 100644
index ee05f8f5..00000000
--- a/internal/event/identifier.go
+++ /dev/null
@@ -1,49 +0,0 @@
-package event
-
-import (
-	"crypto/hmac"
-	"crypto/sha256"
-	"encoding/hex"
-	"fmt"
-	"net"
-
-	"github.com/denisbrodbeck/machineid"
-)
-
-var distinctId string
-
-const (
-	hashKey    = "charm"
-	fallbackId = "unknown"
-)
-
-func getDistinctId() string {
-	if id, err := machineid.ProtectedID(hashKey); err == nil {
-		return id
-	}
-	if macAddr, err := getMacAddr(); err == nil {
-		return hashString(macAddr)
-	}
-	return fallbackId
-}
-
-func getMacAddr() (string, error) {
-	interfaces, err := net.Interfaces()
-	if err != nil {
-		return "", err
-	}
-	for _, iface := range interfaces {
-		if iface.Flags&net.FlagUp != 0 && iface.Flags&net.FlagLoopback == 0 && len(iface.HardwareAddr) > 0 {
-			if addrs, err := iface.Addrs(); err == nil && len(addrs) > 0 {
-				return iface.HardwareAddr.String(), nil
-			}
-		}
-	}
-	return "", fmt.Errorf("no active interface with mac address found")
-}
-
-func hashString(str string) string {
-	hash := hmac.New(sha256.New, []byte(str))
-	hash.Write([]byte(hashKey))
-	return hex.EncodeToString(hash.Sum(nil))
-}
diff --git a/internal/event/logger.go b/internal/event/logger.go
deleted file mode 100644
index 7581676b..00000000
--- a/internal/event/logger.go
+++ /dev/null
@@ -1,28 +0,0 @@
-package event
-
-import (
-	"fmt"
-	"log/slog"
-
-	"github.com/posthog/posthog-go"
-)
-
-var _ posthog.Logger = logger{}
-
-type logger struct{}
-
-func (logger) Debugf(format string, args ...any) {
-	slog.Debug(fmt.Sprintf(format, args...))
-}
-
-func (logger) Logf(format string, args ...any) {
-	slog.Info(fmt.Sprintf(format, args...))
-}
-
-func (logger) Warnf(format string, args ...any) {
-	slog.Warn(fmt.Sprintf(format, args...))
-}
-
-func (logger) Errorf(format string, args ...any) {
-	slog.Error(fmt.Sprintf(format, args...))
-}
diff --git a/internal/log/log.go b/internal/log/log.go
index 9463c3bd..bf99fe60 100644
--- a/internal/log/log.go
+++ b/internal/log/log.go
@@ -9,7 +9,6 @@ import (
 	"sync/atomic"
 	"time"
 
-	"github.com/charmbracelet/crush/internal/event"
 	"gopkg.in/natefinch/lumberjack.v2"
 )
 
@@ -49,8 +48,6 @@ func Initialized() bool {
 
 func RecoverPanic(name string, cleanup func()) {
 	if r := recover(); r != nil {
-		event.Error(r, "panic", true, "name", name)
-
 		// Create a timestamped panic log file
 		timestamp := time.Now().Format("20060102-150405")
 		filename := fmt.Sprintf("crush-panic-%s-%s.log", name, timestamp)
diff --git a/internal/oauth/hyper/device.go b/internal/oauth/hyper/device.go
index 90d115f7..7f10890e 100644
--- a/internal/oauth/hyper/device.go
+++ b/internal/oauth/hyper/device.go
@@ -14,7 +14,6 @@ import (
 	"time"
 
 	"github.com/charmbracelet/crush/internal/agent/hyper"
-	"github.com/charmbracelet/crush/internal/event"
 	"github.com/charmbracelet/crush/internal/oauth"
 )
 
@@ -102,7 +101,6 @@ func PollForToken(ctx context.Context, deviceCode string, expiresIn int) (string
 				return "", err
 			}
 			if result.RefreshToken != "" {
-				event.Alias(result.UserID)
 				return result.RefreshToken, nil
 			}
 			switch result.Error {
diff --git a/internal/session/session.go b/internal/session/session.go
index f9279f9f..20073353 100644
--- a/internal/session/session.go
+++ b/internal/session/session.go
@@ -9,7 +9,6 @@ import (
 	"strings"
 
 	"github.com/charmbracelet/crush/internal/db"
-	"github.com/charmbracelet/crush/internal/event"
 	"github.com/charmbracelet/crush/internal/pubsub"
 	"github.com/google/uuid"
 )
@@ -85,7 +84,6 @@ func (s *service) Create(ctx context.Context, title string) (Session, error) {
 	}
 	session := s.fromDBItem(dbSession)
 	s.Publish(pubsub.CreatedEvent, session)
-	event.SessionCreated()
 	return session, nil
 }
 
@@ -145,7 +143,6 @@ func (s *service) Delete(ctx context.Context, id string) error {
 
 	session := s.fromDBItem(dbSession)
 	s.Publish(pubsub.DeletedEvent, session)
-	event.SessionDeleted()
 	return nil
 }
 
diff --git a/schema.json b/schema.json
index 298d8fe8..692a6e16 100644
--- a/schema.json
+++ b/schema.json
@@ -426,11 +426,6 @@
           "$ref": "#/$defs/Attribution",
           "description": "Attribution settings for generated content"
         },
-        "disable_metrics": {
-          "type": "boolean",
-          "description": "Disable sending metrics",
-          "default": false
-        },
         "initialize_as": {
           "type": "string",
           "description": "Name of the context file to create/update during project initialization",
diff --git a/scripts/install-lsp.sh b/scripts/install-lsp.sh
new file mode 100755
index 00000000..32e8c408
--- /dev/null
+++ b/scripts/install-lsp.sh
@@ -0,0 +1,336 @@
+#!/bin/bash
+#
+# install-lsp.sh - Install and configure LSPs for Crush
+#
+# Usage: ./install-lsp.sh [options]
+#   --force    : Reinstall even if already installed
+#   --dry-run  : Show what would be installed without installing
+#
+
+set -e
+
+FORCE=false
+DRY_RUN=false
+
+for arg in "$@"; do
+    case $arg in
+        --force) FORCE=true ;;
+        --dry-run) DRY_RUN=true ;;
+    esac
+done
+
+echo "Installing LSPs for Crush..."
+
+# Function to check if command exists
+has_command() {
+    command -v "$1" >/dev/null 2>&1
+}
+
+# Function to install via pip
+install_pip() {
+    local pkg="$1"
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would install: pip install $pkg"
+        return
+    fi
+    if has_command pip3; then
+        pip3 install --quiet "$pkg" 2>/dev/null || pip install --quiet "$pkg" 2>/dev/null || true
+    elif has_command pip; then
+        pip install --quiet "$pkg" 2>/dev/null || true
+    fi
+}
+
+# Function to install via npm
+install_npm() {
+    local pkg="$1"
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would install: npm install -g $pkg"
+        return
+    fi
+    if has_command npm; then
+        npm install -g --silent "$pkg" 2>/dev/null || true
+    elif has_command bun; then
+        bun add -g "$pkg" 2>/dev/null || true
+    fi
+}
+
+# Function to install via go
+install_go() {
+    local pkg="$1"
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would install: go install $pkg"
+        return
+    fi
+    if has_command go; then
+        go install "$pkg" 2>/dev/null || true
+    fi
+}
+
+# Function to install via cargo
+install_cargo() {
+    local pkg="$1"
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would install: cargo install $pkg"
+        return
+    fi
+    if has_command cargo; then
+        cargo install "$pkg" 2>/dev/null || true
+    fi
+}
+
+# Function to install via gem
+install_gem() {
+    local pkg="$1"
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would install: gem install $pkg"
+        return
+    fi
+    if has_command gem; then
+        gem install --silent "$pkg" 2>/dev/null || true
+    fi
+}
+
+# Function to add LSP config to crush.json
+add_lsp_config() {
+    local name="$1"
+    local command="$2"
+    local filetypes="$3"
+    local root_markers="$4"
+    shift 4
+    local args="$@"
+
+    if [ "$DRY_RUN" = true ]; then
+        echo "[DRY-RUN] Would add LSP: $name ($command)"
+        return
+    fi
+
+    local config_dir="${HOME}/.config/crush"
+    local config_file="${config_dir}/crush.json"
+
+    # Create config directory if it doesn't exist
+    mkdir -p "$config_dir"
+
+    # Create default config if it doesn't exist
+    if [ ! -f "$config_file" ]; then
+        echo '{}' > "$config_file"
+    fi
+
+    # Check if LSP already exists
+    if grep -q "\"$name\":" "$config_file" 2>/dev/null; then
+        if [ "$FORCE" = true ]; then
+            echo "Updating LSP: $name"
+        else
+            echo "Skipping LSP (already exists): $name"
+            return
+        fi
+    else
+        echo "Adding LSP: $name"
+    fi
+
+    # Use Python to properly merge JSON
+    python3 - "$config_file" "$name" "$command" "$filetypes" "$root_markers" "$args" << 'PYTHON_EOF'
+import json
+import sys
+
+config_file = sys.argv[1]
+name = sys.argv[2]
+command = sys.argv[3]
+filetypes = sys.argv[4].split(',')
+root_markers = sys.argv[5].split(',')
+args = sys.argv[6:] if len(sys.argv) > 6 else []
+
+try:
+    with open(config_file, 'r') as f:
+        config = json.load(f)
+except:
+    config = {}
+
+if 'lsp' not in config:
+    config['lsp'] = {}
+
+config['lsp'][name] = {
+    'command': command,
+    'filetypes': filetypes,
+    'root_markers': root_markers,
+    'args': args
+}
+
+with open(config_file, 'w') as f:
+    json.dump(config, f, indent=2)
+
+print(f"Added {name} to {config_file}")
+PYTHON_EOF
+}
+
+echo ""
+echo "=== Installing Language Servers ==="
+echo ""
+
+# Python (Pyright)
+echo "Checking Python LSP (pyright)..."
+if ! has_command pyright; then
+    install_npm "pyright"
+fi
+
+# JavaScript/TypeScript
+echo "Checking JavaScript/TypeScript LSP (typescript-language-server)..."
+if ! has_command typescript-language-server; then
+    install_npm "typescript-language-server"
+    install_npm "typescript"
+fi
+
+# JSON
+echo "Checking JSON LSP (vscode-json-languageserver)..."
+if ! has_command vscode-json-languageserver; then
+    install_npm "vscode-json-languageserver"
+fi
+
+# HTML/CSS
+echo "Checking HTML/CSS LSP (vscode-html-languageserver)..."
+if ! has_command vscode-html-languageserver; then
+    install_npm "vscode-html-languageserver"
+    install_npm "vscode-css-languageserver"
+fi
+
+# C/C++ (clangd)
+echo "Checking C/C++ LSP (clangd)..."
+if ! has_command clangd; then
+    # Try to install via package manager
+    if has_command apt-get; then
+        [ "$DRY_RUN" = false ] && sudo apt-get install -y clangd 2>/dev/null || true
+    elif has_command dnf; then
+        [ "$DRY_RUN" = false ] && sudo dnf install -y clangd 2>/dev/null || true
+    elif has_command pacman; then
+        [ "$DRY_RUN" = false ] && sudo pacman -S --noconfirm clangd 2>/dev/null || true
+    elif has_command brew; then
+        [ "$DRY_RUN" = false ] && brew install clangd 2>/dev/null || true
+    fi
+fi
+
+# Go (gopls)
+echo "Checking Go LSP (gopls)..."
+if ! has_command gopls; then
+    install_go "golang.org/x/tools/gopls@latest"
+fi
+
+# Rust (rust-analyzer)
+echo "Checking Rust LSP (rust-analyzer)..."
+if ! has_command rust-analyzer; then
+    # Try to install via rustup
+    if has_command rustup; then
+        [ "$DRY_RUN" = false ] && rustup component add rust-analyzer 2>/dev/null || true
+    fi
+fi
+
+# Ruby (solargraph)
+echo "Checking Ruby LSP (solargraph)..."
+if ! has_command solargraph; then
+    install_gem "solargraph"
+fi
+
+# PHP (intelephense)
+echo "Checking PHP LSP (intelephense)..."
+if ! has_command intelephense; then
+    install_npm "intelephense"
+fi
+
+# Java (jdtls)
+echo "Checking Java LSP (jdtls)..."
+if ! has_command jdtls; then
+    # jdtls is complex to install, just note it
+    echo "  Note: jdtls requires manual installation"
+fi
+
+# Lua (lua-language-server)
+echo "Checking Lua LSP (lua-language-server)..."
+if ! has_command lua-language-server; then
+    # Try to install via package manager
+    if has_command apt-get; then
+        [ "$DRY_RUN" = false ] && sudo apt-get install -y lua-language-server 2>/dev/null || true
+    fi
+fi
+
+# YAML
+echo "Checking YAML LSP (yaml-language-server)..."
+if ! has_command yaml-language-server; then
+    install_npm "yaml-language-server"
+fi
+
+# Bash (bash-language-server)
+echo "Checking Bash LSP (bash-language-server)..."
+if ! has_command bash-language-server; then
+    install_npm "bash-language-server"
+fi
+
+# Docker (dockerfile-language-server)
+echo "Checking Docker LSP (dockerfile-language-server)..."
+if ! has_command dockerfile-language-server; then
+    install_npm "dockerfile-language-server-nodejs"
+fi
+
+# Rust (rust-analyzer) - fallback
+if ! has_command rust-analyzer; then
+    install_cargo "rust-analyzer"
+fi
+
+echo ""
+echo "=== Configuring LSPs for Crush ==="
+echo ""
+
+# Python
+add_lsp_config "pyright" "pyright" "python,py" "pyproject.toml,setup.py,setup.cfg,requirements.txt,pyvenv.cfg,.python-version"
+
+# TypeScript/JavaScript
+add_lsp_config "ts_ls" "typescript-language-server" "javascript,typescript,jsx,tsx" "package.json,tsconfig.json,.tsbuildinfo"
+add_lsp_config "json" "vscode-json-languageserver" "json" "package.json"
+
+# HTML/CSS
+add_lsp_config "html" "vscode-html-languageserver" "html" "index.html"
+add_lsp_config "css" "vscode-css-languageserver" "css,scss,less" "package.json"
+
+# C/C++
+add_lsp_config "clangd" "clangd" "c,cpp,objc,objcpp" "compile_commands.json,.clangd,CMakeLists.txt"
+
+# Go
+add_lsp_config "gopls" "gopls" "go,mod" "go.mod,go.sum"
+
+# Rust
+add_lsp_config "rust_analyzer" "rust-analyzer" "rust" "Cargo.toml,rust-toolchain.toml"
+
+# Ruby
+add_lsp_config "solargraph" "solargraph" "ruby" "Gemfile"
+
+# PHP
+add_lsp_config "intelephense" "intelephense" "php" "composer.json"
+
+# Lua
+add_lsp_config "lua" "lua-language-server" "lua" ".luarc.json,.luacheckrc"
+
+# YAML
+add_lsp_config "yaml" "yaml-language-server" "yaml,yml" ".yaml-lint.yml,.eslintrc.yml,docker-compose.yml"
+
+# Bash
+add_lsp_config "bash" "bash-language-server" "bash,sh,zsh" ".bashrc,.zshrc"
+
+# Docker
+add_lsp_config "docker" "dockerfile-language-server" "dockerfile" "Dockerfile"
+
+echo ""
+echo "=== Installation Complete ==="
+echo ""
+echo "Installed LSPs:"
+command -v pyright >/dev/null 2>&1 && echo "  ✓ pyright (Python)"
+command -v typescript-language-server >/dev/null 2>&1 && echo "  ✓ typescript-language-server (JS/TS)"
+command -v clangd >/dev/null 2>&1 && echo "  ✓ clangd (C/C++)"
+command -v gopls >/dev/null 2>&1 && echo "  ✓ gopls (Go)"
+command -v rust-analyzer >/dev/null 2>&1 && echo "  ✓ rust-analyzer (Rust)"
+command -v solargraph >/dev/null 2>&1 && echo "  ✓ solargraph (Ruby)"
+command -v intelephense >/dev/null 2>&1 && echo "  ✓ intelephense (PHP)"
+command -v lua-language-server >/dev/null 2>&1 && echo "  ✓ lua-language-server (Lua)"
+command -v yaml-language-server >/dev/null 2>&1 && echo "  ✓ yaml-language-server (YAML)"
+command -v bash-language-server >/dev/null 2>&1 && echo "  ✓ bash-language-server (Bash)"
+command -v dockerfile-language-server >/dev/null 2>&1 && echo "  ✓ dockerfile-language-server (Docker)"
+echo ""
+echo "Configuration written to: ${HOME}/.config/crush/crush.json"
+echo ""
+echo "Restart Crush to use the new LSPs!"
